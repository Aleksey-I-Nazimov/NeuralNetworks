{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705ad93c-c094-42db-848b-44fbf4f55c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"/home/alex/dev/AiLearning/DataSetStore/text_classification_1/train.csv\")\n",
    "df_valid = pd.read_csv(\"/home/alex/dev/AiLearning/DataSetStore/text_classification_1/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4089aa-9523-474b-862a-8f98344dd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181467 entries, 0 to 181466\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      181467 non-null  int64 \n",
      " 1   text    181467 non-null  object\n",
      " 2   class   181467 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605316c7-0530-4aba-83a3-3d3097c78c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  class\n",
       "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49d21b7-01f3-420f-bf0d-8f58c4a592d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22683 entries, 0 to 22682\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      22683 non-null  int64 \n",
      " 1   text    22683 non-null  object\n",
      " 2   class   22683 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 531.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86aa55d3-adb8-4880-a7e2-07ac1b8dd49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181467</td>\n",
       "      <td>RT @TukvaSociopat: Максимальный репост! ))) #є...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181468</td>\n",
       "      <td>чтоб у меня з.п. ежегодно индексировали на инд...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181469</td>\n",
       "      <td>@chilyandlime нехуя мне не хорошо !!! :((((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181470</td>\n",
       "      <td>@inafish нее , когда ногами ахахах когда?ахаха...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181471</td>\n",
       "      <td>Хочу сделать как лучше,  а получаю как всегда. :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  class\n",
       "0  181467  RT @TukvaSociopat: Максимальный репост! ))) #є...      1\n",
       "1  181468  чтоб у меня з.п. ежегодно индексировали на инд...      0\n",
       "2  181469        @chilyandlime нехуя мне не хорошо !!! :((((      0\n",
       "3  181470  @inafish нее , когда ногами ахахах когда?ахаха...      0\n",
       "4  181471  Хочу сделать как лучше,  а получаю как всегда. :(      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a1e776-a032-4d1f-a42f-8267a61f670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import tokenize as tknz\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Just copy from the homework of the lesson -1\n",
    "class MessageCleaner:\n",
    "    \n",
    "    def __init__(self,message=None):\n",
    "        self._message= message\n",
    "        self.tokens=[]\n",
    "\n",
    "    def set_message(self,msg):\n",
    "        self._message = msg;\n",
    "        return self\n",
    "\n",
    "    def get_message(self):\n",
    "        return self._message\n",
    "\n",
    "    def remove_users(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=\"@[\\w]*\",repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^\\w\\s]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def remove_en_special_sym(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z0-9]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def remove_ru_special_sym(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^а-яА-Я0-9]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "    \n",
    "    def remove_en_numbers(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def remove_ru_numbers(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^а-яА-Я]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def clean_by_regexp(self,regexp: str):\n",
    "        pattern = re.compile(regexp)\n",
    "        xtr = pattern.subn(' ', self._message)[0]\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def to_lower (self ):\n",
    "        self._message = self._message.lower();\n",
    "        return self;\n",
    "\n",
    "    def replace_by_dicts (self,dictionary: map):\n",
    "        for key in dictionary:\n",
    "            self._message = self._message.replace(key,dictionary[key])\n",
    "            #re.sub(pattern=key,repl=dictionary[key],string=self._message)\n",
    "        return self\n",
    "\n",
    "    def escape_single_symbol_words (self):\n",
    "        self._message=\" \".join([word for word in self._message.split() if len(word)>1])\n",
    "        return self\n",
    "\n",
    "    def nltk_word_tokenize(self):\n",
    "        self.tokens=tknz.word_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_word_punc_tokenize(self):\n",
    "        self.tokens = tknz.wordpunct_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_tok_tok_tokenizer(self):\n",
    "        self.tokens = tknz.ToktokTokenizer().tokenize(self._message)\n",
    "        return self\n",
    "        \n",
    "    def nltk_tweet_tokenizer(self):\n",
    "        self.tokens = tknz.TweetTokenizer().tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_with_regexp_tokenizer (self,regexp):\n",
    "        self.tokens = tknz.RegexpTokenizer(regexp).tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_sentence_tokenizer (self):\n",
    "        self.tokens = nltk.sent_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def tokenize (self,tokenizer):\n",
    "        self.tokens=tokenizer(self._message)\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords_from_tokens (self,lang=None,is_new=True):\n",
    "        if lang is None:\n",
    "            sw = set(stopwords.words(\"english\"))\n",
    "        else :\n",
    "            sw = set(stopwords.words(lang))\n",
    "        tks = [token for token in self.tokens if token not in sw]\n",
    "        if is_new:\n",
    "            self.tokens_without_stops = tks\n",
    "        else :\n",
    "            self.tokens = tks\n",
    "        return self\n",
    "\n",
    "    def stemme_tokens (self,stemmer=None,is_new=True):\n",
    "        if stemmer is None:\n",
    "            stemmer = PorterStemmer()\n",
    "        xtr = [stemmer.stem(token) for token in self.tokens]\n",
    "        if is_new :\n",
    "            self.stem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self\n",
    "\n",
    "    def lematize_tokens (self,lematizer=None,is_new=True):\n",
    "        if lematizer is None:\n",
    "            lematizer = WordNetLemmatizer()\n",
    "        xtr = [lematizer.lemmatize(token) for token in self.tokens]\n",
    "        if is_new:\n",
    "            self.lem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self\n",
    "\n",
    "    def process_each_token(self,function):\n",
    "        xtr = [function(token) for token in self.tokens]\n",
    "        self.tokens = xtr\n",
    "        return self\n",
    "\n",
    "    def replace_message_by_sorted_tokens(self,token_array=None):\n",
    "        if token_array is None:\n",
    "            token_array = self.tokens\n",
    "        xtr = \" \".join(sorted(token_array))\n",
    "        self._message = xtr\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50dd26e6-f45e-4f18-8c04-d564e9354e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Segmenter,MorphVocab,Doc,NewsMorphTagger,NewsEmbedding\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_tagger = NewsMorphTagger(NewsEmbedding())\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "def make_natasha_tokens(text,\n",
    "                        morphology_filter_set=None,\n",
    "                        token_length_limit=None,\n",
    "                        empty_token=\"empty\",\n",
    "                        verbose=False):\n",
    "    \n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "\n",
    "    selected_tokens=[]\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "        if verbose:\n",
    "            print (\" >> Input {} \".format(token))\n",
    "        new_token = None\n",
    "        if morphology_filter_set is None:\n",
    "            new_token = token.lemma\n",
    "        else:\n",
    "            if token.pos in morphology_filter_set:\n",
    "                new_token = token.lemma\n",
    "        if new_token is not None and token_length_limit is not None:\n",
    "            if len(new_token)<token_length_limit:\n",
    "                new_token = None\n",
    "        if new_token is not None:\n",
    "            selected_tokens.append(new_token)\n",
    "        if verbose:\n",
    "            print (\" >> Output {} \".format(new_token))\n",
    "    if len(selected_tokens)==0 :\n",
    "        selected_tokens.append(empty_token)\n",
    "\n",
    "    del doc\n",
    "    return selected_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2414a6f-f0f8-40d0-b8f6-a6dd98b9f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def natasha_message_cleaner (text: str):\n",
    "    return MessageCleaner()\\\n",
    "    .set_message(msg=text)\\\n",
    "    .to_lower()\\\n",
    "    .tokenize(tokenizer=lambda txt: make_natasha_tokens(text=txt,\\\n",
    "                                                        morphology_filter_set=('NOUN','VERB','ADJ','PUNCT','PRON'),\\\n",
    "                                                        token_length_limit=None))\\\n",
    "    .tokens\n",
    "\n",
    "def nltk_message_cleaner(text: str):\n",
    "    return MessageCleaner()\\\n",
    "    .set_message(msg=text)\\\n",
    "    .nltk_word_tokenize()\\\n",
    "    .lematize_tokens(is_new=False)\\\n",
    "    .remove_stopwords_from_tokens(lang=\"russian\",is_new=False)\\\n",
    "    .tokens\n",
    "    \n",
    "\n",
    "# .nltk_word_tokenize()\\\n",
    "# .remove_punctuation()\\   \n",
    "# .remove_ru_numbers()\\\n",
    "# .remove_ru_special_sym()\\\n",
    "# .remove_ru_special_sym()\\\n",
    "# .lematize_tokens(is_new=False)\\\n",
    "# .remove_stopwords_from_tokens(lang=\"russian\",is_new=False)\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ebd0be-702a-4377-96d0-2d7cde6a3491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": null, \"filters\": null, \"lower\": null, \"split\": null, \"char_level\": false, \"oov_token\": null, \"document_count\": 0, \"word_counts\": \"{}\", \"word_docs\": \"{}\", \"index_docs\": \"{}\", \"index_word\": \"{}\", \"word_index\": \"{}\"}}\n",
      "Document numbre =  4\n",
      "Word counter:  OrderedDict([('что', 1), ('сказать', 1), (',', 1), ('я', 2), ('видеть', 1), ('кто-то', 1), ('наступить', 1), ('грабли', 1), ('ты', 2), ('разочаровать', 1), ('натравить', 1)])\n",
      "Word into docs counter:  defaultdict(<class 'int'>, {',': 1, 'я': 2, 'сказать': 1, 'видеть': 1, 'что': 1, 'наступить': 1, 'грабли': 1, 'кто-то': 1, 'разочаровать': 1, 'ты': 2, 'натравить': 1})\n",
      "Index docs:  defaultdict(<class 'int'>, {5: 1, 1: 2, 4: 1, 6: 1, 3: 1, 8: 1, 9: 1, 7: 1, 10: 1, 2: 2, 11: 1})\n",
      "Index word:  {1: 'я', 2: 'ты', 3: 'что', 4: 'сказать', 5: ',', 6: 'видеть', 7: 'кто-то', 8: 'наступить', 9: 'грабли', 10: 'разочаровать', 11: 'натравить'}\n",
      "Word index:  {'я': 1, 'ты': 2, 'что': 3, 'сказать': 4, ',': 5, 'видеть': 6, 'кто-то': 7, 'наступить': 8, 'грабли': 9, 'разочаровать': 10, 'натравить': 11}\n",
      "[[3, 4, 5, 1, 6, 7, 8]]\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "\n",
    "#num_words \tthe maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
    "#filters \ta string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
    "#lower \t    boolean. Whether to convert the texts to lowercase.\n",
    "#split \t    str. Separator for word splitting.\n",
    "#char_level if True, every character will be treated as a token.\n",
    "#oov_token \tif given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
    "#analyzer \tfunction. Custom analyzer to split the text. The default analyzer is text_to_word_sequence \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters=None,\n",
    "                      lower=None,\n",
    "                      split=None,\n",
    "                      char_level=False,\n",
    "                      oov_token=None,\n",
    "                      analyzer=natasha_message_cleaner)\n",
    "\n",
    "print(tokenizer.to_json())\n",
    "\n",
    "tokenizer.fit_on_texts([\"Ну что сказать, я вижу\",\"Кто-то наступил на грабли\", \"Ты разочаровал меня\", \"ты был натравлен\"])\n",
    "print(\"Document numbre = \",tokenizer.document_count)\n",
    "print(\"Word counter: \",tokenizer.word_counts)\n",
    "print(\"Word into docs counter: \",tokenizer.word_docs)\n",
    "print(\"Index docs: \",tokenizer.index_docs)\n",
    "print(\"Index word: \",tokenizer.index_word)\n",
    "print(\"Word index: \",tokenizer.word_index)\n",
    "\n",
    "print(tokenizer.texts_to_sequences([\"Ну что сказать, я вижу кто-то наступил\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e88442e-5d66-4ce0-9471-0476f2debcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 93277\n",
      "(181467, 130)\n"
     ]
    }
   ],
   "source": [
    "train_messages = df_train['text'].values\n",
    "valid_messages = df_valid['text'].values\n",
    "\n",
    "ANALYZER = natasha_message_cleaner\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters=None,\n",
    "                      lower=None,\n",
    "                      split=None,\n",
    "                      char_level=False,\n",
    "                      oov_token=None,\n",
    "                      analyzer=ANALYZER)\n",
    "\n",
    "tokenizer.fit_on_texts(train_messages)\n",
    "\n",
    "train_vectors = tokenizer.texts_to_sequences(train_messages)\n",
    "valid_vectors = tokenizer.texts_to_sequences(valid_messages)\n",
    "\n",
    "max_vector_length = max([len(ANALYZER(train_message)) for train_message in train_messages])\n",
    "dictionary_size_plus_1 = len(tokenizer.index_word) + 1\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "# just adding zeroes to vectors with small sizes\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(train_vectors, maxlen=max_vector_length)\n",
    "x_valid = pad_sequences(valid_vectors, maxlen=max_vector_length)\n",
    "y_train = df_train['class'].values\n",
    "y_valid = df_valid['class'].values\n",
    "\n",
    "print (max_vector_length,dictionary_size_plus_1)\n",
    "print (x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96241524-0fe0-4da1-93d9-b4d2dfc6880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 21:59:56.826124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-03 21:59:57.915990: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step\n",
      "(130, 130)\n",
      "(130,)\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/api/layers/core_layers/embedding/\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "#    input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "#    output_dim: Integer. Dimension of the dense embedding.\n",
    "#    embeddings_initializer: Initializer for the embeddings matrix (see keras.initializers).\n",
    "#    embeddings_regularizer: Regularizer function applied to the embeddings matrix (see keras.regularizers).\n",
    "#    embeddings_constraint: Constraint function applied to the embeddings matrix (see keras.constraints).\n",
    "#    mask_zero: Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True, then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).\n",
    "\n",
    "embedding = Embedding(\n",
    "    input_dim=dictionary_size_plus_1,\n",
    "    output_dim=max_vector_length,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    embeddings_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=True\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(embedding)\n",
    "\n",
    "print (model.predict(x_train[0]).shape)\n",
    "print (x_train[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d2405f-1c05-4eae-bba7-664c79cbd9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 130)         12126010  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 130)               33930     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 96)                12576     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 96)                384       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 96)                384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 96)                384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 96)                384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12202085 (46.55 MB)\n",
      "Trainable params: 12201317 (46.54 MB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN,Dropout,Dense,BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "#https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(\n",
    "    input_dim=dictionary_size_plus_1,\n",
    "    output_dim=max_vector_length,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    embeddings_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=True))\n",
    "\n",
    "rnn.add(SimpleRNN(units=max_vector_length,dropout=0.25,recurrent_dropout=0.4))\n",
    "rnn.add(Dense(96, activation='relu'))\n",
    "rnn.add(BatchNormalization())\n",
    "rnn.add(Dropout(0.75))\n",
    "rnn.add(Dense(96, activation='tanh'))\n",
    "rnn.add(BatchNormalization())\n",
    "rnn.add(Dropout(0.5))\n",
    "rnn.add(Dense(96, activation='relu'))\n",
    "rnn.add(BatchNormalization())\n",
    "rnn.add(Dropout(0.25))\n",
    "rnn.add(Dense(96, activation='tanh'))\n",
    "rnn.add(BatchNormalization())\n",
    "rnn.add(Dropout(0.15))\n",
    "rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['accuracy'])\n",
    "rnn.summary()\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopping=EarlyStopping(monitor='val_accuracy',\n",
    "                             baseline=0.70,\n",
    "                             patience=20,\n",
    "                             restore_best_weights=True,\n",
    "                             start_from_epoch=3,\n",
    "                             mode='max',\n",
    "                             verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bda238ce-c0d4-419d-ab65-ae7aecc7dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "73/73 [==============================] - 36s 460ms/step - loss: 0.2574 - accuracy: 0.5521 - val_loss: 0.0973 - val_accuracy: 0.9179\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0723 - accuracy: 0.9013 - val_loss: 0.0538 - val_accuracy: 0.9439\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0222 - accuracy: 0.9729 - val_loss: 0.0360 - val_accuracy: 0.9619\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 34s 459ms/step - loss: 0.0158 - accuracy: 0.9812 - val_loss: 0.0340 - val_accuracy: 0.9640\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0131 - accuracy: 0.9847 - val_loss: 0.0280 - val_accuracy: 0.9703\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 34s 459ms/step - loss: 0.0106 - accuracy: 0.9878 - val_loss: 0.0182 - val_accuracy: 0.9810\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0094 - accuracy: 0.9891 - val_loss: 0.0154 - val_accuracy: 0.9836\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0084 - accuracy: 0.9904 - val_loss: 0.0138 - val_accuracy: 0.9850\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0077 - accuracy: 0.9911 - val_loss: 0.0168 - val_accuracy: 0.9823\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0068 - accuracy: 0.9919 - val_loss: 0.0166 - val_accuracy: 0.9823\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0062 - accuracy: 0.9929 - val_loss: 0.0135 - val_accuracy: 0.9853\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0057 - accuracy: 0.9933 - val_loss: 0.0146 - val_accuracy: 0.9844\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0053 - accuracy: 0.9939 - val_loss: 0.0147 - val_accuracy: 0.9844\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 34s 460ms/step - loss: 0.0052 - accuracy: 0.9941 - val_loss: 0.0132 - val_accuracy: 0.9857\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0049 - accuracy: 0.9945 - val_loss: 0.0115 - val_accuracy: 0.9875\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0046 - accuracy: 0.9947 - val_loss: 0.0114 - val_accuracy: 0.9875\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0044 - accuracy: 0.9950 - val_loss: 0.0121 - val_accuracy: 0.9871\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0041 - accuracy: 0.9953 - val_loss: 0.0129 - val_accuracy: 0.9860\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0038 - accuracy: 0.9957 - val_loss: 0.0119 - val_accuracy: 0.9871\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0037 - accuracy: 0.9958 - val_loss: 0.0139 - val_accuracy: 0.9850\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 33s 457ms/step - loss: 0.0039 - accuracy: 0.9956 - val_loss: 0.0120 - val_accuracy: 0.9872\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0035 - accuracy: 0.9960 - val_loss: 0.0124 - val_accuracy: 0.9867\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0034 - accuracy: 0.9961 - val_loss: 0.0129 - val_accuracy: 0.9861\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0030 - accuracy: 0.9966 - val_loss: 0.0127 - val_accuracy: 0.9868\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0030 - accuracy: 0.9967 - val_loss: 0.0124 - val_accuracy: 0.9867\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0031 - accuracy: 0.9966 - val_loss: 0.0135 - val_accuracy: 0.9857\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0030 - accuracy: 0.9966 - val_loss: 0.0130 - val_accuracy: 0.9861\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0029 - accuracy: 0.9967 - val_loss: 0.0118 - val_accuracy: 0.9874\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0028 - accuracy: 0.9969 - val_loss: 0.0126 - val_accuracy: 0.9864\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0028 - accuracy: 0.9968 - val_loss: 0.0150 - val_accuracy: 0.9841\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0028 - accuracy: 0.9969 - val_loss: 0.0121 - val_accuracy: 0.9869\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 34s 461ms/step - loss: 0.0027 - accuracy: 0.9969 - val_loss: 0.0115 - val_accuracy: 0.9877\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 34s 461ms/step - loss: 0.0026 - accuracy: 0.9972 - val_loss: 0.0110 - val_accuracy: 0.9880\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 34s 461ms/step - loss: 0.0025 - accuracy: 0.9972 - val_loss: 0.0132 - val_accuracy: 0.9861\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 34s 460ms/step - loss: 0.0025 - accuracy: 0.9972 - val_loss: 0.0122 - val_accuracy: 0.9869\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 34s 460ms/step - loss: 0.0026 - accuracy: 0.9971 - val_loss: 0.0120 - val_accuracy: 0.9872\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 34s 460ms/step - loss: 0.0025 - accuracy: 0.9973 - val_loss: 0.0128 - val_accuracy: 0.9863\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 34s 460ms/step - loss: 0.0024 - accuracy: 0.9974 - val_loss: 0.0123 - val_accuracy: 0.9870\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0025 - accuracy: 0.9972 - val_loss: 0.0124 - val_accuracy: 0.9866\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 33s 458ms/step - loss: 0.0026 - accuracy: 0.9971 - val_loss: 0.0128 - val_accuracy: 0.9860\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 34s 460ms/step - loss: 0.0025 - accuracy: 0.9972 - val_loss: 0.0119 - val_accuracy: 0.9874\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0023 - accuracy: 0.9975 - val_loss: 0.0120 - val_accuracy: 0.9870\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0023 - accuracy: 0.9974 - val_loss: 0.0115 - val_accuracy: 0.9877\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0022 - accuracy: 0.9975 - val_loss: 0.0122 - val_accuracy: 0.9871\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0023 - accuracy: 0.9975 - val_loss: 0.0112 - val_accuracy: 0.9880\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0024 - accuracy: 0.9974 - val_loss: 0.0119 - val_accuracy: 0.9871\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 34s 459ms/step - loss: 0.0023 - accuracy: 0.9974 - val_loss: 0.0109 - val_accuracy: 0.9880\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0021 - accuracy: 0.9977 - val_loss: 0.0114 - val_accuracy: 0.9872\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0019 - accuracy: 0.9979 - val_loss: 0.0132 - val_accuracy: 0.9857\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 33s 459ms/step - loss: 0.0020 - accuracy: 0.9978 - val_loss: 0.0139 - val_accuracy: 0.9852\n"
     ]
    }
   ],
   "source": [
    "history = rnn.fit(x_train, y_train,\n",
    "                    batch_size=2000,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6152bee-0543-4280-b71a-5a36e4abca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 2s 35ms/step - loss: 0.0151 - accuracy: 0.9840\n",
      "[0.01509775873273611, 0.9839527606964111]\n",
      "Test score: 0.01509775873273611\n",
      "Test accuracy: 0.9839527606964111\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(x_valid, y_valid, batch_size=512, verbose=1)\n",
    "print (score)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de0ba756-55c9-45c6-b4a1-af0b8d47b575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 130)         12126010  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 130)               135720    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 96)                12576     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 96)                384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 96)                384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 96)                384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 96)                9312      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 96)                384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12303875 (46.94 MB)\n",
      "Trainable params: 12303107 (46.93 MB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dropout,Dense\n",
    "\n",
    "# https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(\n",
    "    input_dim=dictionary_size_plus_1,\n",
    "    output_dim=max_vector_length,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    embeddings_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=True))\n",
    "\n",
    "lstm.add(LSTM(units=max_vector_length,dropout=0.3,activation=\"tanh\",\n",
    "    recurrent_activation=\"tanh\",recurrent_dropout=0.3))\n",
    "lstm.add(Dense(96, activation='relu'))\n",
    "lstm.add(BatchNormalization())\n",
    "lstm.add(Dropout(0.65))\n",
    "lstm.add(Dense(96, activation='tanh'))\n",
    "lstm.add(BatchNormalization())\n",
    "lstm.add(Dropout(0.45))\n",
    "lstm.add(Dense(96, activation='relu'))\n",
    "lstm.add(BatchNormalization())\n",
    "lstm.add(Dropout(0.20))\n",
    "lstm.add(Dense(96, activation='tanh'))\n",
    "lstm.add(BatchNormalization())\n",
    "lstm.add(Dropout(0.15))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm.summary()\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopping=EarlyStopping(monitor='val_accuracy',\n",
    "                             baseline=0.70,\n",
    "                             patience=20,\n",
    "                             restore_best_weights=True,\n",
    "                             start_from_epoch=3,\n",
    "                             mode='max',\n",
    "                             verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d35a3-5309-4100-8c4e-09c2038dda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "73/73 [==============================] - 97s 1s/step - loss: 0.2176 - accuracy: 0.8960 - val_loss: 0.9404 - val_accuracy: 0.4930\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 93s 1s/step - loss: 0.0387 - accuracy: 0.9869 - val_loss: 1.1506 - val_accuracy: 0.4930\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 93s 1s/step - loss: 0.0205 - accuracy: 0.9938 - val_loss: 1.0157 - val_accuracy: 0.4935\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 93s 1s/step - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.7832 - val_accuracy: 0.5312\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 93s 1s/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.3158 - val_accuracy: 0.8207\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 93s 1s/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0996 - val_accuracy: 0.9640\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 93s 1s/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0344 - val_accuracy: 0.9883\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 93s 1s/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0384 - val_accuracy: 0.9872\n",
      "Epoch 9/50\n",
      "66/73 [==========================>...] - ETA: 8s - loss: 0.0059 - accuracy: 0.9982"
     ]
    }
   ],
   "source": [
    "history = lstm.fit(x_train, y_train,\n",
    "                    batch_size=2000,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a965fef-6fba-464b-8f26-a225ab1b467e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b95777-5f8d-4f46-b613-a6c641ec2cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb0dee-e97a-4b93-80bb-ec379617c0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03e3b6-329a-490c-af2a-4d8545abddf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55d15b-6ab2-4c37-9d5b-1b8b3772cc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399599d3-6f2a-49c3-8159-75b1574aa77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20c402-fcf0-48d8-a4d9-c5ab32e859f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ceed9-de27-4908-9137-fdd4e09758d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
