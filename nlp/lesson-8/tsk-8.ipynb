{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705ad93c-c094-42db-848b-44fbf4f55c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"/home/alex/dev/AiLearning/DataSetStore/text_classification_1/train.csv\")\n",
    "df_valid = pd.read_csv(\"/home/alex/dev/AiLearning/DataSetStore/text_classification_1/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4089aa-9523-474b-862a-8f98344dd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181467 entries, 0 to 181466\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      181467 non-null  int64 \n",
      " 1   text    181467 non-null  object\n",
      " 2   class   181467 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605316c7-0530-4aba-83a3-3d3097c78c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  class\n",
       "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49d21b7-01f3-420f-bf0d-8f58c4a592d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22683 entries, 0 to 22682\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      22683 non-null  int64 \n",
      " 1   text    22683 non-null  object\n",
      " 2   class   22683 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 531.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86aa55d3-adb8-4880-a7e2-07ac1b8dd49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181467</td>\n",
       "      <td>RT @TukvaSociopat: Максимальный репост! ))) #є...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181468</td>\n",
       "      <td>чтоб у меня з.п. ежегодно индексировали на инд...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181469</td>\n",
       "      <td>@chilyandlime нехуя мне не хорошо !!! :((((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181470</td>\n",
       "      <td>@inafish нее , когда ногами ахахах когда?ахаха...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181471</td>\n",
       "      <td>Хочу сделать как лучше,  а получаю как всегда. :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  class\n",
       "0  181467  RT @TukvaSociopat: Максимальный репост! ))) #є...      1\n",
       "1  181468  чтоб у меня з.п. ежегодно индексировали на инд...      0\n",
       "2  181469        @chilyandlime нехуя мне не хорошо !!! :((((      0\n",
       "3  181470  @inafish нее , когда ногами ахахах когда?ахаха...      0\n",
       "4  181471  Хочу сделать как лучше,  а получаю как всегда. :(      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a1e776-a032-4d1f-a42f-8267a61f670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import tokenize as tknz\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Just copy from the homework of the lesson -1\n",
    "class MessageCleaner:\n",
    "    \n",
    "    def __init__(self,message=None):\n",
    "        self._message= message\n",
    "        self.tokens=[]\n",
    "\n",
    "    def set_message(self,msg):\n",
    "        self._message = msg;\n",
    "        return self\n",
    "\n",
    "    def get_message(self):\n",
    "        return self._message\n",
    "\n",
    "    def remove_users(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=\"@[\\w]*\",repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^\\w\\s]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def remove_en_special_sym(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z0-9]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def remove_ru_special_sym(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^а-яА-Я0-9]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "    \n",
    "    def remove_en_numbers(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def remove_ru_numbers(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^а-яА-Я]',repl=replace,string=self._message)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def clean_by_regexp(self,regexp: str):\n",
    "        pattern = re.compile(regexp)\n",
    "        xtr = pattern.subn(' ', self._message)[0]\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def to_lower (self ):\n",
    "        self._message = self._message.lower();\n",
    "        return self;\n",
    "\n",
    "    def replace_by_dicts (self,dictionary: map):\n",
    "        for key in dictionary:\n",
    "            self._message = self._message.replace(key,dictionary[key])\n",
    "            #re.sub(pattern=key,repl=dictionary[key],string=self._message)\n",
    "        return self\n",
    "\n",
    "    def escape_single_symbol_words (self):\n",
    "        self._message=\" \".join([word for word in self._message.split() if len(word)>1])\n",
    "        return self\n",
    "\n",
    "    def nltk_word_tokenize(self):\n",
    "        self.tokens=tknz.word_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_word_punc_tokenize(self):\n",
    "        self.tokens = tknz.wordpunct_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_tok_tok_tokenizer(self):\n",
    "        self.tokens = tknz.ToktokTokenizer().tokenize(self._message)\n",
    "        return self\n",
    "        \n",
    "    def nltk_tweet_tokenizer(self):\n",
    "        self.tokens = tknz.TweetTokenizer().tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_with_regexp_tokenizer (self,regexp):\n",
    "        self.tokens = tknz.RegexpTokenizer(regexp).tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_sentence_tokenizer (self):\n",
    "        self.tokens = nltk.sent_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords_from_tokens (self,lang=None,is_new=True):\n",
    "        if lang is None:\n",
    "            sw = set(stopwords.words(\"english\"))\n",
    "        else :\n",
    "            sw = set(stopwords.words(lang))\n",
    "        tks = [token for token in self.tokens if token not in sw]\n",
    "        if is_new:\n",
    "            self.tokens_without_stops = tks\n",
    "        else :\n",
    "            self.tokens = tks\n",
    "        return self\n",
    "\n",
    "    def stemme_tokens (self,stemmer=None,is_new=True):\n",
    "        if stemmer is None:\n",
    "            stemmer = PorterStemmer()\n",
    "        xtr = [stemmer.stem(token) for token in self.tokens]\n",
    "        if is_new :\n",
    "            self.stem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self\n",
    "\n",
    "    def lematize_tokens (self,lematizer=None,is_new=True):\n",
    "        if lematizer is None:\n",
    "            lematizer = WordNetLemmatizer()\n",
    "        xtr = [lematizer.lemmatize(token) for token in self.tokens]\n",
    "        if is_new:\n",
    "            self.lem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self\n",
    "\n",
    "    def replace_message_by_sorted_tokens(self,token_array=None):\n",
    "        if token_array is None:\n",
    "            token_array = self.tokens\n",
    "        xtr = \" \".join(sorted(token_array))\n",
    "        self._message = xtr\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2414a6f-f0f8-40d0-b8f6-a6dd98b9f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_message_cleaner (text: str):\n",
    "    return MessageCleaner()\\\n",
    "    .set_message(msg=text)\\\n",
    "    .remove_punctuation()\\\n",
    "    .remove_ru_special_sym()\\\n",
    "    .remove_ru_numbers()\\\n",
    "    .escape_single_symbol_words()\\\n",
    "    .to_lower()\\\n",
    "    .nltk_word_tokenize()\\\n",
    "    .lematize_tokens(is_new=False)\\\n",
    "    .tokens\n",
    "\n",
    "#    .remove_stopwords_from_tokens(lang=\"russian\",is_new=False)\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45ebd0be-702a-4377-96d0-2d7cde6a3491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": null, \"filters\": null, \"lower\": null, \"split\": null, \"char_level\": false, \"oov_token\": null, \"document_count\": 0, \"word_counts\": \"{}\", \"word_docs\": \"{}\", \"index_docs\": \"{}\", \"index_word\": \"{}\", \"word_index\": \"{}\"}}\n",
      "Document numbre =  4\n",
      "Word counter:  OrderedDict([('ну', 1), ('что', 1), ('сказать', 1), ('вижу', 1), ('кто', 1), ('то', 1), ('наступил', 1), ('на', 1), ('грабли', 1), ('ты', 2), ('разочаровал', 1), ('меня', 1), ('был', 1), ('натравлен', 1)])\n",
      "Word into docs counter:  defaultdict(<class 'int'>, {'что': 1, 'ну': 1, 'сказать': 1, 'вижу': 1, 'кто': 1, 'грабли': 1, 'на': 1, 'наступил': 1, 'то': 1, 'ты': 2, 'разочаровал': 1, 'меня': 1, 'натравлен': 1, 'был': 1})\n",
      "Index docs:  defaultdict(<class 'int'>, {3: 1, 2: 1, 4: 1, 5: 1, 6: 1, 10: 1, 9: 1, 8: 1, 7: 1, 1: 2, 11: 1, 12: 1, 14: 1, 13: 1})\n",
      "Index word:  {1: 'ты', 2: 'ну', 3: 'что', 4: 'сказать', 5: 'вижу', 6: 'кто', 7: 'то', 8: 'наступил', 9: 'на', 10: 'грабли', 11: 'разочаровал', 12: 'меня', 13: 'был', 14: 'натравлен'}\n",
      "Word index:  {'ты': 1, 'ну': 2, 'что': 3, 'сказать': 4, 'вижу': 5, 'кто': 6, 'то': 7, 'наступил': 8, 'на': 9, 'грабли': 10, 'разочаровал': 11, 'меня': 12, 'был': 13, 'натравлен': 14}\n",
      "[[2, 3, 4, 5, 6, 7, 8]]\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "\n",
    "#num_words \tthe maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
    "#filters \ta string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
    "#lower \t    boolean. Whether to convert the texts to lowercase.\n",
    "#split \t    str. Separator for word splitting.\n",
    "#char_level if True, every character will be treated as a token.\n",
    "#oov_token \tif given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
    "#analyzer \tfunction. Custom analyzer to split the text. The default analyzer is text_to_word_sequence \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters=None,\n",
    "                      lower=None,\n",
    "                      split=None,\n",
    "                      char_level=False,\n",
    "                      oov_token=None,\n",
    "                      analyzer=apply_message_cleaner)\n",
    "\n",
    "print(tokenizer.to_json())\n",
    "\n",
    "tokenizer.fit_on_texts([\"Ну что сказать, я вижу\",\"Кто-то наступил на грабли\", \"Ты разочаровал меня\", \"ты был натравлен\"])\n",
    "print(\"Document numbre = \",tokenizer.document_count)\n",
    "print(\"Word counter: \",tokenizer.word_counts)\n",
    "print(\"Word into docs counter: \",tokenizer.word_docs)\n",
    "print(\"Index docs: \",tokenizer.index_docs)\n",
    "print(\"Index word: \",tokenizer.index_word)\n",
    "print(\"Word index: \",tokenizer.word_index)\n",
    "\n",
    "print(tokenizer.texts_to_sequences([\"Ну что сказать, я вижу кто-то наступил\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e88442e-5d66-4ce0-9471-0476f2debcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 148854\n",
      "(181467, 27)\n"
     ]
    }
   ],
   "source": [
    "train_messages = df_train['text'].values\n",
    "valid_messages = df_valid['text'].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters=None,\n",
    "                      lower=None,\n",
    "                      split=None,\n",
    "                      char_level=False,\n",
    "                      oov_token=None,\n",
    "                      analyzer=apply_message_cleaner)\n",
    "\n",
    "tokenizer.fit_on_texts(train_messages)\n",
    "\n",
    "train_vectors = tokenizer.texts_to_sequences(train_messages)\n",
    "valid_vectors = tokenizer.texts_to_sequences(valid_messages)\n",
    "\n",
    "max_vector_length = max([len(apply_message_cleaner(train_message)) for train_message in train_messages])\n",
    "dictionary_size_plus_1 = len(tokenizer.index_word) + 1\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "# just adding zeroes to vectors with small sizes\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(train_vectors, maxlen=max_vector_length)\n",
    "x_valid = pad_sequences(valid_vectors, maxlen=max_vector_length)\n",
    "y_train = df_train['class'].values\n",
    "y_valid = df_valid['class'].values\n",
    "\n",
    "print (max_vector_length,dictionary_size_plus_1)\n",
    "print (x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96241524-0fe0-4da1-93d9-b4d2dfc6880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "(27, 27)\n",
      "(27,)\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/api/layers/core_layers/embedding/\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "#    input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "#    output_dim: Integer. Dimension of the dense embedding.\n",
    "#    embeddings_initializer: Initializer for the embeddings matrix (see keras.initializers).\n",
    "#    embeddings_regularizer: Regularizer function applied to the embeddings matrix (see keras.regularizers).\n",
    "#    embeddings_constraint: Constraint function applied to the embeddings matrix (see keras.constraints).\n",
    "#    mask_zero: Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True, then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).\n",
    "\n",
    "embedding = Embedding(\n",
    "    input_dim=dictionary_size_plus_1,\n",
    "    output_dim=max_vector_length,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    embeddings_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=True\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(embedding)\n",
    "\n",
    "print (model.predict(x_train[0]).shape)\n",
    "print (x_train[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47d2405f-1c05-4eae-bba7-664c79cbd9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, None, 27)          4019058   \n",
      "                                                                 \n",
      " simple_rnn_14 (SimpleRNN)   (None, 96)                11904     \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 96)                9312      \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 96)                0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 96)                9312      \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 96)                0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 96)                9312      \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 96)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 96)                9312      \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 96)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 1)                 97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4068307 (15.52 MB)\n",
      "Trainable params: 4068307 (15.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN,Dropout,Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "#https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(\n",
    "    input_dim=dictionary_size_plus_1,\n",
    "    output_dim=max_vector_length,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    embeddings_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=True))\n",
    "\n",
    "rnn.add(SimpleRNN(units=96,dropout=0.25,recurrent_dropout=0.4))\n",
    "rnn.add(Dense(96, activation='relu'))\n",
    "rnn.add(Dropout(0.75))\n",
    "rnn.add(Dense(96, activation='tanh'))\n",
    "rnn.add(Dropout(0.5))\n",
    "rnn.add(Dense(96, activation='relu'))\n",
    "rnn.add(Dropout(0.25))\n",
    "rnn.add(Dense(96, activation='tanh'))\n",
    "rnn.add(Dropout(0.15))\n",
    "rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['accuracy'])\n",
    "rnn.summary()\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopping=EarlyStopping(monitor='val_accuracy',\n",
    "                             baseline=0.70,\n",
    "                             patience=20,\n",
    "                             restore_best_weights=True,\n",
    "                             start_from_epoch=10,\n",
    "                             mode='max',\n",
    "                             verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bda238ce-c0d4-419d-ab65-ae7aecc7dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "73/73 [==============================] - 6s 66ms/step - loss: 0.2526 - accuracy: 0.5025 - val_loss: 0.2502 - val_accuracy: 0.4930\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 5s 64ms/step - loss: 0.2510 - accuracy: 0.5067 - val_loss: 0.2505 - val_accuracy: 0.4930\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 5s 64ms/step - loss: 0.2504 - accuracy: 0.5086 - val_loss: 0.2504 - val_accuracy: 0.4930\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 5s 64ms/step - loss: 0.2499 - accuracy: 0.5182 - val_loss: 0.2510 - val_accuracy: 0.4930\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 5s 65ms/step - loss: 0.2495 - accuracy: 0.5269 - val_loss: 0.2517 - val_accuracy: 0.4930\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.2466 - accuracy: 0.5519 - val_loss: 0.2385 - val_accuracy: 0.6020\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.2150 - accuracy: 0.6692 - val_loss: 0.1964 - val_accuracy: 0.7095\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.1641 - accuracy: 0.7717 - val_loss: 0.1874 - val_accuracy: 0.7287\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.1281 - accuracy: 0.8305 - val_loss: 0.1930 - val_accuracy: 0.7252\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.1068 - accuracy: 0.8620 - val_loss: 0.2073 - val_accuracy: 0.7208\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0936 - accuracy: 0.8807 - val_loss: 0.2067 - val_accuracy: 0.7244\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0840 - accuracy: 0.8934 - val_loss: 0.2132 - val_accuracy: 0.7195\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0762 - accuracy: 0.9039 - val_loss: 0.2152 - val_accuracy: 0.7151\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0707 - accuracy: 0.9118 - val_loss: 0.2204 - val_accuracy: 0.7180\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0662 - accuracy: 0.9179 - val_loss: 0.2187 - val_accuracy: 0.7200\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0618 - accuracy: 0.9236 - val_loss: 0.2239 - val_accuracy: 0.7184\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0594 - accuracy: 0.9268 - val_loss: 0.2319 - val_accuracy: 0.7122\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0560 - accuracy: 0.9315 - val_loss: 0.2304 - val_accuracy: 0.7168\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0541 - accuracy: 0.9337 - val_loss: 0.2318 - val_accuracy: 0.7170\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0515 - accuracy: 0.9373 - val_loss: 0.2299 - val_accuracy: 0.7148\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0496 - accuracy: 0.9401 - val_loss: 0.2349 - val_accuracy: 0.7132\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0478 - accuracy: 0.9424 - val_loss: 0.2359 - val_accuracy: 0.7125\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0471 - accuracy: 0.9429 - val_loss: 0.2398 - val_accuracy: 0.7127\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0451 - accuracy: 0.9457 - val_loss: 0.2385 - val_accuracy: 0.7154\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0434 - accuracy: 0.9475 - val_loss: 0.2409 - val_accuracy: 0.7131\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0429 - accuracy: 0.9485 - val_loss: 0.2427 - val_accuracy: 0.7117\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0416 - accuracy: 0.9502 - val_loss: 0.2420 - val_accuracy: 0.7122\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0407 - accuracy: 0.9516 - val_loss: 0.2432 - val_accuracy: 0.7105\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0394 - accuracy: 0.9527 - val_loss: 0.2436 - val_accuracy: 0.7119\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0387 - accuracy: 0.9537 - val_loss: 0.2395 - val_accuracy: 0.7111\n",
      "Epoch 31/50\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9546Restoring model weights from the end of the best epoch: 11.\n",
      "73/73 [==============================] - 5s 66ms/step - loss: 0.0382 - accuracy: 0.9545 - val_loss: 0.2416 - val_accuracy: 0.7084\n",
      "Epoch 31: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = rnn.fit(x_train, y_train,\n",
    "                    batch_size=2000,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6152bee-0543-4280-b71a-5a36e4abca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2066 - accuracy: 0.7237\n",
      "[0.2066294550895691, 0.7237138152122498]\n",
      "Test score: 0.2066294550895691\n",
      "Test accuracy: 0.7237138152122498\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(x_valid, y_valid, batch_size=512, verbose=1)\n",
    "print (score)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de0ba756-55c9-45c6-b4a1-af0b8d47b575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, None, 27)          4019058   \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 27)                5940      \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 250)               7000      \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_86 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_87 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_88 (Dropout)        (None, 250)               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4220499 (16.10 MB)\n",
      "Trainable params: 4220499 (16.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dropout,Dense\n",
    "\n",
    "# https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(\n",
    "    input_dim=dictionary_size_plus_1,\n",
    "    output_dim=max_vector_length,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    embeddings_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=True))\n",
    "\n",
    "lstm.add(LSTM(units=27,dropout=0.3,activation=\"tanh\",\n",
    "    recurrent_activation=\"tanh\",recurrent_dropout=0.3))\n",
    "lstm.add(Dense(250, activation='relu'))\n",
    "lstm.add(Dropout(0.65))\n",
    "lstm.add(Dense(250, activation='tanh'))\n",
    "lstm.add(Dropout(0.45))\n",
    "lstm.add(Dense(250, activation='relu'))\n",
    "lstm.add(Dropout(0.20))\n",
    "lstm.add(Dense(250, activation='tanh'))\n",
    "lstm.add(Dropout(0.15))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm.summary()\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "early_stopping=EarlyStopping(monitor='val_accuracy',\n",
    "                             baseline=0.70,\n",
    "                             patience=15,\n",
    "                             restore_best_weights=True,\n",
    "                             start_from_epoch=10,\n",
    "                             mode='max',\n",
    "                             verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d35a3-5309-4100-8c4e-09c2038dda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "73/73 [==============================] - 7s 76ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5070\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.6931 - accuracy: 0.5055 - val_loss: 0.6931 - val_accuracy: 0.5070\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.6607 - accuracy: 0.5853 - val_loss: 0.5784 - val_accuracy: 0.6942\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.4873 - accuracy: 0.7714 - val_loss: 0.5323 - val_accuracy: 0.7342\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.3482 - accuracy: 0.8531 - val_loss: 0.5829 - val_accuracy: 0.7286\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.2680 - accuracy: 0.8930 - val_loss: 0.6431 - val_accuracy: 0.7229\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.2201 - accuracy: 0.9128 - val_loss: 0.7321 - val_accuracy: 0.7144\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.1877 - accuracy: 0.9261 - val_loss: 0.8038 - val_accuracy: 0.7132\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.1637 - accuracy: 0.9353 - val_loss: 0.8782 - val_accuracy: 0.7110\n",
      "Epoch 10/50\n",
      "39/73 [===============>..............] - ETA: 2s - loss: 0.1350 - accuracy: 0.9463"
     ]
    }
   ],
   "source": [
    "history = lstm.fit(x_train, y_train,\n",
    "                    batch_size=2000,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a965fef-6fba-464b-8f26-a225ab1b467e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b95777-5f8d-4f46-b613-a6c641ec2cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb0dee-e97a-4b93-80bb-ec379617c0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03e3b6-329a-490c-af2a-4d8545abddf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55d15b-6ab2-4c37-9d5b-1b8b3772cc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399599d3-6f2a-49c3-8159-75b1574aa77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20c402-fcf0-48d8-a4d9-c5ab32e859f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ceed9-de27-4908-9137-fdd4e09758d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
