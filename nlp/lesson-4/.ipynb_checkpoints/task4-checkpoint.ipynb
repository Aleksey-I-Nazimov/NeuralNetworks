{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71842a57-e531-4a4d-ba4b-84111e615373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import tokenize as tknz\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger\n",
    "\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab871b8c-a5e9-4d2c-ab49-a7fad704350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \n",
    "    def __init__(self,message=None,verbose=False):\n",
    "        self._message= message\n",
    "        self._verbose=verbose\n",
    "        self.tokens=[]\n",
    "\n",
    "    def set_message(self,msg):\n",
    "        self._message = msg;\n",
    "        if self._verbose: print (\"  --> The message was set {}\".format(self._message))\n",
    "        return self\n",
    "\n",
    "    def get_message(self):\n",
    "        return self._message\n",
    "\n",
    "    def remove_email_word(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=\"@[\\w]*\",repl=replace,string=self._message)\n",
    "        if self._verbose: print(\"  --> The @word was removed {} {} \".format(xtr,self._message))\n",
    "        return self.set_message(msg=xtr)\n",
    "    \n",
    "    def remove_punctuation(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^\\w\\s]',repl=replace,string=self._message)\n",
    "        if self._verbose: print(\"  --> Removing punctuation: {} {}\".format(xtr,self._message))\n",
    "        return self.set_message(msg=xtr)\n",
    "\n",
    "    def remove_special_sym(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z0-9]',repl=replace,string=self._message)\n",
    "        if self._verbose: print (\"  --> Removing special symbols: {} {}\".format(xtr,self._message))\n",
    "        return self.set_message(msg=xtr)\n",
    "    \n",
    "    def remove_numbers(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z]',repl=replace,string=self._message)\n",
    "        if self._verbose: print(\"  --> Removing all numbers: {} {}\".format(xtr,self._message))\n",
    "        return self.set_message(msg=xtr)\n",
    "\n",
    "    def to_lower (self ):\n",
    "        xtr = self._message.lower();\n",
    "        if self._verbose: print (\"  --> Making to lower case: {} {}\".format(xtr,self._message))\n",
    "        return self.set_message(msg=xtr)\n",
    "\n",
    "    def replace_by_dicts (self,dictionary: dict):\n",
    "        for key in dictionary:\n",
    "            xtr = self._message.replace(key,dictionary[key])\n",
    "            #re.sub(pattern=key,repl=dictionary[key],string=self._message)\n",
    "            if self._verbose:\n",
    "                print (\"  --> Replacing: {}->{} Result: before: {} after: {}\".format(key,dictionary[key],self._message,xtr))\n",
    "            self.set_message(msg=xtr)\n",
    "        return self\n",
    "\n",
    "    def escape_single_symbol_words (self):\n",
    "        xtr=\" \".join([word for word in self._message.split() if len(word)>1])\n",
    "        if self._verbose:\n",
    "            print (\"  --> Escaping single symbol words: {} {}\".format(xtr,self._message))\n",
    "        return self.set_message(msg=xtr)\n",
    "\n",
    "    def make_tokenization(self,tokenizer):\n",
    "        self.tokens=tokenizer(self._message)\n",
    "        if self._verbose:\n",
    "            print (\"  --> custom tokenizer was completed: {}\".format(self.tokens))\n",
    "        return self\n",
    "\n",
    "    def nltk_word_tokenize(self):\n",
    "        self.tokens=tknz.word_tokenize(self._message)\n",
    "        if self._verbose:\n",
    "            print (\"  --> nltk.tokenize.word_tokenize was completed: {}\".format(self.tokens))\n",
    "        return self\n",
    "\n",
    "    def nltk_word_punc_tokenize(self):\n",
    "        self.tokens = tknz.wordpunct_tokenize(self._message)\n",
    "        if self._verbose:\n",
    "            print (\"  --> nltk.tokenize.wordpunct_tokenize was completed: {}\".format(self.tokens))\n",
    "        return self\n",
    "\n",
    "    def nltk_tok_tok_tokenizer(self):\n",
    "        self.tokens = tknz.ToktokTokenizer().tokenize(self._message)\n",
    "        if self._verbose:\n",
    "            print (\"  --> nltk.tokenize.ToktokTokenizer().tokenize was completed: {}\".format(self.tokens))\n",
    "        return self\n",
    "        \n",
    "    def nltk_tweet_tokenizer(self):\n",
    "        self.tokens = tknz.TweetTokenizer().tokenize(self._message)\n",
    "        if self._verbose:\n",
    "            print (\"  --> nltk.tokenize.TweetTokenizer().tokenize was completed: {}\".format(self.tokens))\n",
    "        return self\n",
    "\n",
    "    def nltk_with_regexp_tokenizer (self,regexp):\n",
    "        self.tokens = tknz.RegexpTokenizer(regexp).tokenize(self._message)\n",
    "        if self._verbose:\n",
    "            print (\"  --> nltk.tokenize.RegexpTokenizer({}).tokenize was completed: {}\".format(regexp,self.tokens))\n",
    "        return self\n",
    "\n",
    "    def nltk_sentence_tokenizer (self):\n",
    "        self.tokens = nltk.sent_tokenize(self._message)\n",
    "        if self._verbose:\n",
    "            print (\"  --> nltk.sent_tokenize() was completed: {}\".format(self.tokens))\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords_from_tokens (self,lang=None,is_new=True):\n",
    "        if lang is None:\n",
    "            sw = set(stopwords.words(\"english\"))\n",
    "        else :\n",
    "            sw = set(stopwords.words(lang))\n",
    "        tks = [token for token in self.tokens if token not in sw]\n",
    "        if is_new:\n",
    "            self.tokens_without_stops = tks\n",
    "        else :\n",
    "            self.tokens = tks\n",
    "        return self\n",
    "\n",
    "    def stemme_tokens (self,stemmer=None,is_new=True):\n",
    "        if stemmer is None:\n",
    "            stemmer = PorterStemmer()\n",
    "        xtr = [stemmer.stem(token) for token in self.tokens]\n",
    "        if is_new :\n",
    "            self.stem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self\n",
    "\n",
    "    def lematize_tokens (self,lematizer=None,is_new=True):\n",
    "        if lematizer is None:\n",
    "            lematizer = WordNetLemmatizer()\n",
    "        xtr = [lematizer.lemmatize(token) for token in self.tokens]\n",
    "        if is_new:\n",
    "            self.lem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f787603f-0908-4d76-8c47-5fe01c9756c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path_1 = \"/home/alex/dev/AiLearning/DataSetStore/twitter_messages_2/negative.csv\"\n",
    "df1 = pd.read_csv(file_path_1,header=None,on_bad_lines='skip',sep=\";\")\n",
    "\n",
    "file_path_2 = \"/home/alex/dev/AiLearning/DataSetStore/twitter_messages_2/positive.csv\"\n",
    "df2 = pd.read_csv(file_path_2,header=None,on_bad_lines='skip',sep=\";\")\n",
    "\n",
    "df = df1._append(df2)\n",
    "df.head()\n",
    "df_copy = df.copy()[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b896c48-e728-4b3f-ba9b-33dfd9444969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Input DocToken(stop=5, text='Желаю', pos='VERB', feats=<Imp,Ind,Sing,1,Pres,Fin,Act>, lemma='желать') \n",
      " >> Output желать \n",
      " >> Input DocToken(start=6, stop=14, text='хорошего', pos='ADJ', feats=<Gen,Pos,Masc,Sing>, lemma='хороший') \n",
      " >> Output None \n",
      " >> Input DocToken(start=15, stop=21, text='полёта', pos='NOUN', feats=<Inan,Gen,Masc,Sing>, lemma='полет') \n",
      " >> Output полет \n",
      " >> Input DocToken(start=22, stop=23, text='и', pos='CCONJ', lemma='и') \n",
      " >> Output None \n",
      " >> Input DocToken(start=24, stop=31, text='удачной', pos='ADJ', feats=<Gen,Pos,Fem,Sing>, lemma='удачный') \n",
      " >> Output None \n",
      " >> Input DocToken(start=32, stop=39, text='посадки', pos='NOUN', feats=<Inan,Gen,Fem,Sing>, lemma='посадка') \n",
      " >> Output посадка \n",
      " >> Input DocToken(start=39, stop=40, text=',', pos='PUNCT', lemma=',') \n",
      " >> Output None \n",
      " >> Input DocToken(start=40, stop=41, text='я', pos='PRON', feats=<Nom,Sing,1>, lemma='я') \n",
      " >> Output None \n",
      " >> Input DocToken(start=42, stop=46, text='буду', pos='AUX', feats=<Imp,Ind,Sing,1,Pres,Fin,Act>, lemma='быть') \n",
      " >> Output None \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['желать', 'полет', 'посадка']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def make_natasha_tokens(text,\n",
    "                        morphology_filter_set=None,\n",
    "                        token_length_limit=None,\n",
    "                        empty_token=\"empty\",\n",
    "                        verbose=False):\n",
    "    segmenter = Segmenter()\n",
    "    morph_tagger = NewsMorphTagger(NewsEmbedding())\n",
    "    morph_vocab = MorphVocab()\n",
    "    \n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "\n",
    "    selected_tokens=[]\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "        if verbose:\n",
    "            print (\" >> Input {} \".format(token))\n",
    "        new_token = None\n",
    "        if morphology_filter_set is None:\n",
    "            new_token = token.lemma\n",
    "        else:\n",
    "            if token.pos in morphology_filter_set:\n",
    "                new_token = token.lemma\n",
    "        if new_token is not None and token_length_limit is not None:\n",
    "            if len(new_token)<token_length_limit:\n",
    "                new_token = None\n",
    "        if new_token is not None:\n",
    "            selected_tokens.append(new_token)\n",
    "        if verbose:\n",
    "            print (\" >> Output {} \".format(new_token))\n",
    "    if len(selected_tokens)==0 :\n",
    "        selected_tokens.append(empty_token)\n",
    "    return selected_tokens\n",
    "\n",
    "make_natasha_tokens(text=\"Желаю хорошего полёта и удачной посадки,я буду\",morphology_filter_set=('NOUN','VERB'), token_length_limit=3,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb65aa1f-cdc4-45bf-be2b-531a4f5cbb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> The message was set Желаю хорошего полёта и удачной посадки,я буду\n",
      "  --> The @word was removed Желаю хорошего полёта и удачной посадки,я буду Желаю хорошего полёта и удачной посадки,я буду \n",
      "  --> The message was set Желаю хорошего полёта и удачной посадки,я буду\n",
      "  --> Removing punctuation: Желаю хорошего полёта и удачной посадки я буду Желаю хорошего полёта и удачной посадки,я буду\n",
      "  --> The message was set Желаю хорошего полёта и удачной посадки я буду\n",
      "  --> Making to lower case: желаю хорошего полёта и удачной посадки я буду Желаю хорошего полёта и удачной посадки я буду\n",
      "  --> The message was set желаю хорошего полёта и удачной посадки я буду\n",
      "  --> Escaping single symbol words: желаю хорошего полёта удачной посадки буду желаю хорошего полёта и удачной посадки я буду\n",
      "  --> The message was set желаю хорошего полёта удачной посадки буду\n",
      "  --> custom tokenizer was completed: ['желать', 'полет', 'посадка']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['желать', 'полет', 'посадка']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process(txt: str,verbose=False):\n",
    "    return TextProcessor(verbose=verbose)\\\n",
    "    .set_message(msg=txt)\\\n",
    "    .remove_email_word()\\\n",
    "    .remove_punctuation()\\\n",
    "    .to_lower()\\\n",
    "    .escape_single_symbol_words()\\\n",
    "    .remove_stopwords_from_tokens(lang='russian',is_new=False)\\\n",
    "    .make_tokenization(tokenizer=lambda txt: make_natasha_tokens(text=txt,morphology_filter_set=('NOUN','VERB'),token_length_limit=3))\\\n",
    "    .tokens\n",
    "    \n",
    "\n",
    "process(txt=\"Желаю хорошего полёта и удачной посадки,я буду\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53730bc-dca3-4c99-b717-27b7e8aaec2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408906762813579264</td>\n",
       "      <td>1386325944</td>\n",
       "      <td>dugarchikbellko</td>\n",
       "      <td>на работе был полный пиддес :| и так каждое за...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8064</td>\n",
       "      <td>111</td>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "      <td>[работа, пиддес, закрытие, месяц, свихнуться]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408906818262687744</td>\n",
       "      <td>1386325957</td>\n",
       "      <td>nugemycejela</td>\n",
       "      <td>Коллеги сидят рубятся в Urban terror, а я из-з...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>[коллега, сидеть, рубятся, винд, мочь]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408906858515398656</td>\n",
       "      <td>1386325966</td>\n",
       "      <td>4post21</td>\n",
       "      <td>@elina_4post как говорят обещаного три года жд...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>718</td>\n",
       "      <td>49</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>[говорить, обещаного, год, ждать]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>408906914437685248</td>\n",
       "      <td>1386325980</td>\n",
       "      <td>Poliwake</td>\n",
       "      <td>Желаю хорошего полёта и удачной посадки,я буду...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10628</td>\n",
       "      <td>207</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>[желать, полет, посадка, скучать]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408906914723295232</td>\n",
       "      <td>1386325980</td>\n",
       "      <td>capyvixowe</td>\n",
       "      <td>Обновил за каким-то лешим surf, теперь не рабо...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>[обновить, леший, работать, простоплеер]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1                2  \\\n",
       "0  408906762813579264  1386325944  dugarchikbellko   \n",
       "1  408906818262687744  1386325957     nugemycejela   \n",
       "2  408906858515398656  1386325966          4post21   \n",
       "3  408906914437685248  1386325980         Poliwake   \n",
       "4  408906914723295232  1386325980       capyvixowe   \n",
       "\n",
       "                                                   3  4  5  6  7      8    9  \\\n",
       "0  на работе был полный пиддес :| и так каждое за... -1  0  0  0   8064  111   \n",
       "1  Коллеги сидят рубятся в Urban terror, а я из-з... -1  0  0  0     26   42   \n",
       "2  @elina_4post как говорят обещаного три года жд... -1  0  0  0    718   49   \n",
       "3  Желаю хорошего полёта и удачной посадки,я буду... -1  0  0  0  10628  207   \n",
       "4  Обновил за каким-то лешим surf, теперь не рабо... -1  0  0  0     35   17   \n",
       "\n",
       "    10  11                                         tokens  \n",
       "0   94   2  [работа, пиддес, закрытие, месяц, свихнуться]  \n",
       "1   39   0         [коллега, сидеть, рубятся, винд, мочь]  \n",
       "2  249   0              [говорить, обещаного, год, ждать]  \n",
       "3  200   0              [желать, полет, посадка, скучать]  \n",
       "4   34   0       [обновить, леший, работать, простоплеер]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy['tokens'] = df_copy[3].apply(lambda txt: process(txt=txt,verbose=False))\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d156eec8-6ba8-4763-8fd5-e74ae0640f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from gensim) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m846.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12cf7f0-88fc-4a3c-af77-18f0d3597edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import *\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f93862-33b1-4438-9d9e-d55fff04d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(df_copy['tokens'])\n",
    "\n",
    "dictionary.filter_extremes(no_below = 10, no_above = 0.9, keep_n=None) \n",
    "dictionary.save('lenta.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdc5809-cc1c-45d1-8be5-53e084e9b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in df_copy['tokens']]\n",
    "corpora.MmCorpus.serialize('lenta.model', corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f7727d-f912-456e-9fb1-a11addf48020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.2 ms, sys: 1.97 ms, total: 49.1 ms\n",
      "Wall time: 84 ms\n"
     ]
    }
   ],
   "source": [
    "%time lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=20, chunksize=50, update_every=1, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa8de5a-d7fd-47eb-af3d-459ac9b23e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (12,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (16,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (10,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (2,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (15,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (6,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (19,\n",
       "  '0.311*\"хотеть\" + 0.194*\"нет\" + 0.165*\"мочь\" + 0.165*\"день\" + 0.165*\"любить\"'),\n",
       " (17,\n",
       "  '0.200*\"любить\" + 0.200*\"мочь\" + 0.200*\"день\" + 0.200*\"хотеть\" + 0.200*\"нет\"'),\n",
       " (9,\n",
       "  '0.987*\"нет\" + 0.003*\"мочь\" + 0.003*\"хотеть\" + 0.003*\"день\" + 0.003*\"любить\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics(num_topics=10, num_words=10, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32aa562-94f5-4a26-9d31-443b00d2d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (1.26.1)\n",
      "Requirement already satisfied: scipy in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (1.11.3)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (2.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (3.1.2)\n",
      "Collecting numexpr (from pyLDAvis)\n",
      "  Downloading numexpr-2.8.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: gensim in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pyLDAvis) (66.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from gensim->pyLDAvis) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/alex/ai_learn_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Downloading numexpr-2.8.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (377 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.2/377.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: funcy, numexpr, pyLDAvis\n",
      "Successfully installed funcy-2.0 numexpr-2.8.8 pyLDAvis-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1458920a-66c0-4323-bd0e-523e8d67722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.2 ms, sys: 0 ns, total: 80.2 ms\n",
      "Wall time: 540 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/ai_learn_env/lib/python3.11/site-packages/sklearn/manifold/_mds.py:298: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el20231403236563126244966003911\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el20231403236563126244966003911_data = {\"mdsDat\": {\"x\": [0.3579432679599632, 0.27749515535336766, -0.32561194503615604, -0.15074749824804268, -0.14821148423420266, -0.0017160276963174655, -0.0017160276963174683, -0.0017160276963174752, -0.0017160276963174793, -0.0017160276963174668, -0.0017160276963174613, -0.0017160276963174863, -0.0017160276963174821, -0.0017160276963174806, -0.0017160276963174848, -0.0017160276963174793, -0.0017160276963174737, -0.0017160276963174848, -0.0017160276963174737, 0.013156891953515116], \"y\": [-0.16791146472885915, 0.2908216124377145, 0.23236754205497723, -0.29871249593464094, -0.33090553462895284, 0.018365716830465253, 0.018365716830465246, 0.018365716830465267, 0.018365716830465267, 0.018365716830465253, 0.01836571683046525, 0.01836571683046528, 0.018365716830465274, 0.01836571683046528, 0.01836571683046528, 0.018365716830465274, 0.018365716830465256, 0.01836571683046528, 0.018365716830465263, 0.017220305173247605], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [17.70830023227304, 14.857423224415653, 10.032862548765944, 8.061455415501873, 7.399636791027621, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.796021453187951, 2.7960214433845563]}, \"tinfo\": {\"Term\": [\"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043b\\u044e\\u0431\\u0438\\u0442\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0434\\u0435\\u043d\\u044c\"], \"Freq\": [17.0, 20.0, 17.0, 13.0, 6.0, 13.333758291117798, 0.031138258687220966, 0.031138258687220966, 0.031138258687220966, 0.031138258687220966, 11.143113269431641, 0.037132282634767866, 0.037132282634767866, 0.037132282634767866, 0.037132282634767866, 7.501589632382716, 0.029537257379171893, 0.03477450900483567, 0.029537257379171893, 0.029537257379171893, 5.8713375587211125, 0.06384214962585472, 0.06384214962585472, 0.06384214962585472, 0.06384214962585472, 5.5069420249693435, 0.029295710535260797, 0.029162070447379868, 0.029162070447379868, 0.029162070447379868, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.42499539516754004, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.4249952368444555, 0.3504329473041362, 0.6611093216687094, 0.3504329473041362, 0.4125682031782808, 0.3504329473041362], \"Total\": [17.0, 20.0, 17.0, 13.0, 6.0, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 17.6645317761976, 6.491314138511975, 13.96323065690446, 17.776453645518135, 20.10447468869607, 13.96323065690446, 6.491314138511975, 17.6645317761976, 17.776453645518135, 20.10447468869607, 17.776453645518135, 6.491314138511975, 13.96323065690446, 17.6645317761976, 20.10447468869607, 17.776453645518135, 6.491314138511975, 13.96323065690446, 17.6645317761976, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 13.96323065690446, 17.6645317761976, 17.776453645518135, 20.10447468869607, 6.491314138511975, 20.10447468869607, 13.96323065690446, 17.6645317761976, 17.776453645518135], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"logprob\": [5.0, 4.0, 3.0, 2.0, 1.0, -0.0093, -6.0689, -6.0689, -6.0689, -6.0689, -0.0132, -5.7173, -5.7173, -5.7173, -5.7173, -0.0163, -5.5535, -5.3903, -5.5535, -5.5535, -0.0426, -4.564, -4.564, -4.564, -4.564, -0.021, -5.2573, -5.2619, -5.2619, -5.2619, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.6094, -1.8023, -1.1676, -1.8023, -1.6391, -1.8023], \"loglift\": [5.0, 4.0, 3.0, 2.0, 1.0, 1.3205, -3.6086, -4.3746, -4.6097, -4.6161, 1.4459, -3.2571, -4.023, -4.2645, -4.3875, 1.678, -3.0933, -3.9311, -4.1007, -4.2237, 1.4103, -2.1037, -2.8697, -3.1048, -3.2342, 1.4319, -2.797, -3.5676, -3.8027, -3.9321, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.8508, 0.0849, -0.1503, -0.1566, -0.2796, 0.6579, 0.1622, -0.108, -0.1799, -0.3495]}, \"token.table\": {\"Topic\": [4, 5, 3, 2, 1, 20], \"Freq\": [0.33752513969583253, 0.33752513969583253, 0.5729333129682423, 0.6227167603062174, 0.6466222172574035, 0.04974017055826181], \"Term\": [\"\\u0434\\u0435\\u043d\\u044c\", \"\\u0434\\u0435\\u043d\\u044c\", \"\\u043c\\u043e\\u0447\\u044c\", \"\\u043d\\u0435\\u0442\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\", \"\\u0445\\u043e\\u0442\\u0435\\u0442\\u044c\"]}, \"R\": 5, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 10, 1, 14, 12, 4, 2, 3, 8, 7, 9, 5, 11, 13, 15, 16, 17, 18, 19, 20]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el20231403236563126244966003911\", ldavis_el20231403236563126244966003911_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el20231403236563126244966003911\", ldavis_el20231403236563126244966003911_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el20231403236563126244966003911\", ldavis_el20231403236563126244966003911_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "\n",
    "%time vis_data = gensimvis.prepare(lda, corpus, dictionary, mds='mmds')\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd018c1-5e62-484a-b391-3c9fb3b91dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
