{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffd9941-7352-453a-8709-e3faf941276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 17:09:37.551666: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 17:09:37.552853: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-30 17:09:37.573076: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 17:09:37.573093: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 17:09:37.573107: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 17:09:37.576807: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-30 17:09:37.577268: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 17:09:38.219487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd26dd90-0db1-4d04-bd99-45c6a8e3dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"/home/alex/dev/AiLearning/DataSetStore/translation/rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efcd934-75cd-4e1d-b0ed-25e699f26186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w: str) -> str:\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w\n",
    "\n",
    "\n",
    "def read_lines(file_path: str, limit=None) -> list:\n",
    "    # TODO: Possible random selection or train split here\n",
    "    return list(io.open(path_to_file, encoding='UTF-8').read().split('\\n')[:limit])\n",
    "\n",
    "def split_by_tab(line_list: list, split_limit=2) -> list:\n",
    "    result = list()\n",
    "    for line in line_list:\n",
    "        splitted_line = line.split('\\t')\n",
    "        if len(splitted_line)>=split_limit:\n",
    "            result.append(splitted_line[:split_limit])\n",
    "    return result\n",
    "    #return [[w for w in line.split('\\t')[:split_limit]]  for line in line_list]\n",
    "\n",
    "def apply_preprocess(line_2d_list: list) -> tuple:\n",
    "    line_2d_list = [[preprocess_sentence(line) for line in line_list]  for line_list in line_2d_list]\n",
    "    return zip(*line_2d_list)\n",
    "\n",
    "def make_preprocessing(file_path: str, line_limit=None):\n",
    "    return apply_preprocess(\n",
    "        line_2d_list=split_by_tab(\n",
    "            line_list=read_lines(\n",
    "                file_path=file_path,limit=line_limit)))\n",
    "\n",
    "def tokenize(text_list: list) -> tuple:\n",
    "  # overriding default filters and building internal courpus\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(text_list)\n",
    "    \n",
    "  # building numerical sequence. Each word is coded as the respective corpus number\n",
    "  tensor = lang_tokenizer.texts_to_sequences(text_list)\n",
    "\n",
    "  # messages have different lenght. Padding allows to make tensors with same length\n",
    "  # length difference is compensated by adding ZEROES before or after\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "\n",
    "def load_dataset(file_path: str, line_limit=None):\n",
    "\n",
    "  targ_lang, inp_lang = make_preprocessing(file_path=file_path,\n",
    "                                           line_limit=line_limit)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(text_list=inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(text_list=targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa4aa6d-90a7-4d9d-a131-15aafa8efff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 8000 2000 2000\n",
      "(64, 12) (64, 8)\n",
      "8 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 17:09:44.138611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-30 17:09:44.151460: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 10000\n",
    "# Making tensors\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(file_path=path_to_file,line_limit=LIMIT)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print (example_input_batch.shape, example_target_batch.shape)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "print (max_length_targ,max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7983366-8aa0-4a12-881b-8015c9f6b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 12, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9499519-1b3f-4b35-b164-748d7a25cc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7865afd9-ef33-4ba6-ba27-502a752325e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 1734)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74430b48-b49d-44c4-8815-b26b4c3f5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and the loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "# Checkpoints (Object-based saving)\n",
    "checkpoint_dir = './training_attention_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5b0f84-4708-4500-91d6-ea3d5d9f26db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.2680\n",
      "Epoch 1 Batch 100 Loss 1.7110\n",
      "Epoch 1 Loss 2.1529\n",
      "Time taken for 1 epoch 41.12944173812866 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.6583\n",
      "Epoch 2 Batch 100 Loss 1.4366\n",
      "Epoch 2 Loss 1.5484\n",
      "Time taken for 1 epoch 35.18496656417847 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2740\n",
      "Epoch 3 Batch 100 Loss 1.2172\n",
      "Epoch 3 Loss 1.2887\n",
      "Time taken for 1 epoch 33.91944456100464 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.1850\n",
      "Epoch 4 Batch 100 Loss 0.9782\n",
      "Epoch 4 Loss 1.0799\n",
      "Time taken for 1 epoch 35.06866717338562 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9637\n",
      "Epoch 5 Batch 100 Loss 0.9693\n",
      "Epoch 5 Loss 0.9092\n",
      "Time taken for 1 epoch 33.96410632133484 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.7811\n",
      "Epoch 6 Batch 100 Loss 0.7371\n",
      "Epoch 6 Loss 0.7590\n",
      "Time taken for 1 epoch 35.00062370300293 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.6188\n",
      "Epoch 7 Batch 100 Loss 0.5787\n",
      "Epoch 7 Loss 0.6243\n",
      "Time taken for 1 epoch 33.94570446014404 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.5087\n",
      "Epoch 8 Batch 100 Loss 0.4830\n",
      "Epoch 8 Loss 0.5054\n",
      "Time taken for 1 epoch 35.31659984588623 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.3993\n",
      "Epoch 9 Batch 100 Loss 0.3889\n",
      "Epoch 9 Loss 0.3923\n",
      "Time taken for 1 epoch 33.98506236076355 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2356\n",
      "Epoch 10 Batch 100 Loss 0.3478\n",
      "Epoch 10 Loss 0.3020\n",
      "Time taken for 1 epoch 35.296279191970825 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.2082\n",
      "Epoch 11 Batch 100 Loss 0.2700\n",
      "Epoch 11 Loss 0.2307\n",
      "Time taken for 1 epoch 33.98406219482422 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.1585\n",
      "Epoch 12 Batch 100 Loss 0.1673\n",
      "Epoch 12 Loss 0.1753\n",
      "Time taken for 1 epoch 35.37545895576477 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.1169\n",
      "Epoch 13 Batch 100 Loss 0.1571\n",
      "Epoch 13 Loss 0.1346\n",
      "Time taken for 1 epoch 33.96426963806152 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0755\n",
      "Epoch 14 Batch 100 Loss 0.1268\n",
      "Epoch 14 Loss 0.1063\n",
      "Time taken for 1 epoch 35.21988582611084 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0796\n",
      "Epoch 15 Batch 100 Loss 0.1071\n",
      "Epoch 15 Loss 0.0888\n",
      "Time taken for 1 epoch 33.97412967681885 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.1046\n",
      "Epoch 16 Batch 100 Loss 0.0455\n",
      "Epoch 16 Loss 0.0778\n",
      "Time taken for 1 epoch 35.167866468429565 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0729\n",
      "Epoch 17 Batch 100 Loss 0.0738\n",
      "Epoch 17 Loss 0.0699\n",
      "Time taken for 1 epoch 34.006868839263916 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0291\n",
      "Epoch 18 Batch 100 Loss 0.0857\n",
      "Epoch 18 Loss 0.0656\n",
      "Time taken for 1 epoch 35.17765760421753 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0620\n",
      "Epoch 19 Batch 100 Loss 0.0619\n",
      "Epoch 19 Loss 0.0654\n",
      "Time taken for 1 epoch 34.0073516368866 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0434\n",
      "Epoch 20 Batch 100 Loss 0.0625\n",
      "Epoch 20 Loss 0.0602\n",
      "Time taken for 1 epoch 35.15534472465515 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0500\n",
      "Epoch 21 Batch 100 Loss 0.0456\n",
      "Epoch 21 Loss 0.0579\n",
      "Time taken for 1 epoch 34.06160354614258 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0424\n",
      "Epoch 22 Batch 100 Loss 0.0656\n",
      "Epoch 22 Loss 0.0541\n",
      "Time taken for 1 epoch 35.078290700912476 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0526\n",
      "Epoch 23 Batch 100 Loss 0.0510\n",
      "Epoch 23 Loss 0.0542\n",
      "Time taken for 1 epoch 34.009876012802124 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0495\n",
      "Epoch 24 Batch 100 Loss 0.0463\n",
      "Epoch 24 Loss 0.0517\n",
      "Time taken for 1 epoch 35.052199363708496 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0195\n",
      "Epoch 25 Batch 100 Loss 0.0471\n",
      "Epoch 25 Loss 0.0522\n",
      "Time taken for 1 epoch 33.93341517448425 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0948\n",
      "Epoch 26 Batch 100 Loss 0.0764\n",
      "Epoch 26 Loss 0.0553\n",
      "Time taken for 1 epoch 35.067261695861816 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0313\n",
      "Epoch 27 Batch 100 Loss 0.0684\n",
      "Epoch 27 Loss 0.0519\n",
      "Time taken for 1 epoch 33.956772565841675 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0331\n",
      "Epoch 28 Batch 100 Loss 0.0788\n",
      "Epoch 28 Loss 0.0539\n",
      "Time taken for 1 epoch 35.14248728752136 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0374\n",
      "Epoch 29 Batch 100 Loss 0.0472\n",
      "Epoch 29 Loss 0.0537\n",
      "Time taken for 1 epoch 33.97532868385315 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0825\n",
      "Epoch 30 Batch 100 Loss 0.1009\n",
      "Epoch 30 Loss 0.0587\n",
      "Time taken for 1 epoch 35.162312030792236 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0205\n",
      "Epoch 31 Batch 100 Loss 0.0557\n",
      "Epoch 31 Loss 0.0610\n",
      "Time taken for 1 epoch 33.97233986854553 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0452\n",
      "Epoch 32 Batch 100 Loss 0.0548\n",
      "Epoch 32 Loss 0.0622\n",
      "Time taken for 1 epoch 35.3502197265625 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0403\n",
      "Epoch 33 Batch 100 Loss 0.0507\n",
      "Epoch 33 Loss 0.0577\n",
      "Time taken for 1 epoch 33.95912480354309 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0382\n",
      "Epoch 34 Batch 100 Loss 0.0616\n",
      "Epoch 34 Loss 0.0523\n",
      "Time taken for 1 epoch 35.072020053863525 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0182\n",
      "Epoch 35 Batch 100 Loss 0.0754\n",
      "Epoch 35 Loss 0.0478\n",
      "Time taken for 1 epoch 33.93427658081055 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0347\n",
      "Epoch 36 Batch 100 Loss 0.0501\n",
      "Epoch 36 Loss 0.0454\n",
      "Time taken for 1 epoch 35.12446069717407 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0421\n",
      "Epoch 37 Batch 100 Loss 0.0427\n",
      "Epoch 37 Loss 0.0444\n",
      "Time taken for 1 epoch 33.97750425338745 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0292\n",
      "Epoch 38 Batch 100 Loss 0.0526\n",
      "Epoch 38 Loss 0.0412\n",
      "Time taken for 1 epoch 35.13295292854309 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0183\n",
      "Epoch 39 Batch 100 Loss 0.0421\n",
      "Epoch 39 Loss 0.0388\n",
      "Time taken for 1 epoch 33.93996572494507 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0146\n",
      "Epoch 40 Batch 100 Loss 0.0246\n",
      "Epoch 40 Loss 0.0378\n",
      "Time taken for 1 epoch 35.14092564582825 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0118\n",
      "Epoch 41 Batch 100 Loss 0.0405\n",
      "Epoch 41 Loss 0.0364\n",
      "Time taken for 1 epoch 33.9486985206604 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0532\n",
      "Epoch 42 Batch 100 Loss 0.0351\n",
      "Epoch 42 Loss 0.0358\n",
      "Time taken for 1 epoch 35.116267681121826 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0143\n",
      "Epoch 43 Batch 100 Loss 0.0455\n",
      "Epoch 43 Loss 0.0355\n",
      "Time taken for 1 epoch 33.96369457244873 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0256\n",
      "Epoch 44 Batch 100 Loss 0.0656\n",
      "Epoch 44 Loss 0.0354\n",
      "Time taken for 1 epoch 35.28225922584534 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0289\n",
      "Epoch 45 Batch 100 Loss 0.0450\n",
      "Epoch 45 Loss 0.0360\n",
      "Time taken for 1 epoch 34.26270508766174 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0217\n",
      "Epoch 46 Batch 100 Loss 0.0492\n",
      "Epoch 46 Loss 0.0354\n",
      "Time taken for 1 epoch 35.38309288024902 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0198\n",
      "Epoch 47 Batch 100 Loss 0.0374\n",
      "Epoch 47 Loss 0.0415\n",
      "Time taken for 1 epoch 34.14949297904968 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0277\n",
      "Epoch 48 Batch 100 Loss 0.0492\n",
      "Epoch 48 Loss 0.0596\n",
      "Time taken for 1 epoch 35.22294592857361 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0459\n",
      "Epoch 49 Batch 100 Loss 0.1108\n",
      "Epoch 49 Loss 0.0779\n",
      "Time taken for 1 epoch 33.98621344566345 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0966\n",
      "Epoch 50 Batch 100 Loss 0.0822\n",
      "Epoch 50 Loss 0.0701\n",
      "Time taken for 1 epoch 35.25826454162598 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca22b85-3681-4e2d-9f48-3fa6441da3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7100b419-8069-46c1-8fc1-54805093f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs: https://docs.python-telegram-bot.org/en/stable/examples.echobot.html\n",
    "\n",
    "from datetime import datetime\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from telegram import ForceReply, Update\n",
    "from telegram.ext import Application, CommandHandler, ContextTypes, MessageHandler, filters\n",
    "\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
    "    user = update.effective_user\n",
    "    await update.message.reply_html(\n",
    "        rf\"Hi {user.mention_html()}!\",\n",
    "        reply_markup=ForceReply(selective=True)\n",
    "    )\n",
    "\n",
    "async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"Send a message when the command /help is issued.\"\"\"\n",
    "    await update.message.reply_text(\"Help!\")\n",
    "\n",
    "async def make_answer(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    print (\"Question: \",update.message.text)\n",
    "    r,s,a=evaluate(update.message.text)\n",
    "    print (\"Answer: \",r)\n",
    "    await update.message.reply_text(r)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # Create the Application and pass it your bot's token.\n",
    "    application = Application.builder().token(\"6626784673:AAEVV3uyouH-BhMNyyoXEnrEANPDskd-DO4\").build()\n",
    "\n",
    "    # on different commands - answer in Telegram\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    application.add_handler(CommandHandler(\"help\", help_command))\n",
    "\n",
    "    # on non command i.e message - echo the message on Telegram\n",
    "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, make_answer))\n",
    "\n",
    "    # Run the bot until the user presses Ctrl-C\n",
    "    application.run_polling(allowed_updates=Update.ALL_TYPES)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269abe5e-4dc8-43f2-87b0-bf28d9aff8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  идти\n",
      "Answer:  do you go ! <end> \n",
      "Question:  иду\n",
      "Answer:  i'm coming . <end> \n",
      "Question:  читаю\n",
      "Answer:  i'm sorry . <end> \n",
      "Question:  ем\n",
      "Answer:  i'm working . <end> \n",
      "Question:  сплю\n",
      "Answer:  terrific ! <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/ai_learn_env/lib/python3.11/site-packages/telegram/ext/_application.py\", line 1234, in process_update\n",
      "    await coroutine\n",
      "  File \"/home/alex/ai_learn_env/lib/python3.11/site-packages/telegram/ext/_basehandler.py\", line 157, in handle_update\n",
      "    return await self.callback(update, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13218/166713138.py\", line 25, in make_answer\n",
      "    r,s,a=evaluate(update.message.text)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13218/346849971.py\", line 5, in evaluate\n",
      "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13218/346849971.py\", line 5, in <listcomp>\n",
      "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
      "              ~~~~~~~~~~~~~~~~~~~^^^\n",
      "KeyError: 'гляжу'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  гляжу\n",
      "Question:  вижу\n",
      "Answer:  may i hurry ? <end> \n",
      "Question:  На помощь\n",
      "Answer:  this is ? <end> \n",
      "Question:  Пригнись\n",
      "Answer:  duck ! <end> \n",
      "Question:  Здорово\n",
      "Answer:  that's closed . <end> \n",
      "Question:  Кто\n",
      "Answer:  who are men . <end> \n",
      "Question:  Приветик\n",
      "Answer:  hello . <end> \n",
      "Question:  Здорово\n",
      "Answer:  that's closed . <end> \n",
      "Question:  Идите\n",
      "Answer:  go of <end> \n",
      "Question:  пожар\n",
      "Answer:  fire ! <end> \n",
      "Question:  спасите\n",
      "Answer:  i got lucky . <end> \n",
      "Question:  помоги\n",
      "Answer:  help fixed . <end> \n",
      "Question:  прячься\n",
      "Answer:  hurry up . <end> \n",
      "Question:  остановись\n",
      "Answer:  stop ! <end> \n",
      "Question:  подожди\n",
      "Answer:  how quit ? <end> \n",
      "Question:  начинай\n",
      "Answer:  shut up . <end> \n",
      "Question:  ого\n",
      "Answer:  that's lame . <end> \n",
      "Question:  я побежал\n",
      "Answer:  i swore . <end> \n",
      "Question:  понимаю\n",
      "Answer:  where is it ? <end> \n",
      "Question:  как быть\n",
      "Answer:  how strange ! <end> \n",
      "Question:  быть или не быть\n",
      "Answer:  it's hailing . <end> \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af52d45-9d00-45fa-b197-c84ad94bd538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd815e7-3f55-4aed-8a34-fce2ee117cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
