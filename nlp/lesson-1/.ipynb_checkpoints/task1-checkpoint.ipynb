{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516fba67-b41b-4587-91ac-1e70b7ba783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path_1 = \"/home/alex/dev/AiLearning/DataSetStore/twitter_messages/test.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "\n",
    "file_path_2 = \"/home/alex/dev/AiLearning/DataSetStore/twitter_messages/train.csv\"\n",
    "\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd571d45-6b9c-4655-9967-e99ca6adafc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import tokenize as tknz\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c8d56c4-27be-4d64-8112-2f1883e70c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageCleaner:\n",
    "    \n",
    "    def __init__(self,message=None):\n",
    "        self._message= message\n",
    "        self.tokens=[]\n",
    "        # self.reg_map = {\"removing_users\":\"@[\\w]*\",\n",
    "        #                 \"punctuation\":'[%s]',#'\\\\[^\\w\\s]',\n",
    "        #                 \"special_symbols\":\"\\\\[^a-zA-Z0-9]\",\n",
    "        #                 \"numbers_to_spaces\":\"\\\\[^a-zA-Z]\" }\n",
    "\n",
    "    def set_message(self,msg):\n",
    "        self._message = msg;\n",
    "        return self\n",
    "\n",
    "    def get_message(self):\n",
    "        return self._message\n",
    "\n",
    "    def remove_users(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=\"@[\\w]*\",repl=replace,string=self._message)\n",
    "        #print (\"USER\",self._message,xtr)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^\\w\\s]',repl=replace,string=self._message)\n",
    "        #print (\"PUNCT\",self._message,xtr)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    def remove_special_sym(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z0-9]',repl=replace,string=self._message)\n",
    "        #print (\"SS\",self._message,xtr)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "    \n",
    "    def remove_numbers(self,replace=\" \"):\n",
    "        xtr = re.sub(pattern=r'[^a-zA-Z]',repl=replace,string=self._message)\n",
    "        #print (\"NN\",self._message,xtr)\n",
    "        self._message = xtr\n",
    "        return self\n",
    "\n",
    "    # def replace_by_regs (self,replace_map=None):\n",
    "    #     for reg_key in self.reg_map:\n",
    "    #         regexp = self.reg_map[reg_key]\n",
    "    #         if replace_map != None and reg_key in replace_map:\n",
    "    #             replace = replace_map[reg_key]\n",
    "    #         else :\n",
    "    #             replace = \" \"\n",
    "    #         xtr = re.sub(pattern=regexp,repl=replace,string=self._message)\n",
    "    #         print (regexp,self._message,xtr)\n",
    "    #         self._message = xtr\n",
    "    #     return self\n",
    "\n",
    "    def to_lower (self ):\n",
    "        self._message = self._message.lower();\n",
    "        return self;\n",
    "\n",
    "    def replace_by_dicts (self,dictionary: map):\n",
    "        for key in dictionary:\n",
    "            self._message = self._message.replace(key,dictionary[key])\n",
    "            #re.sub(pattern=key,repl=dictionary[key],string=self._message)\n",
    "        return self\n",
    "\n",
    "    def escape_single_symbol_words (self):\n",
    "        self._message=\" \".join([word for word in self._message.split() if len(word)>1])\n",
    "        return self\n",
    "\n",
    "    def nltk_word_tokenize(self):\n",
    "        #print (\"1\",self._message)\n",
    "        self.tokens=tknz.word_tokenize(self._message)\n",
    "        #print (\"2\",self.tokens)\n",
    "        return self\n",
    "\n",
    "    def nltk_word_punc_tokenize(self):\n",
    "        self.tokens = tknz.wordpunct_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_tok_tok_tokenizer(self):\n",
    "        self.tokens = tknz.ToktokTokenizer().tokenize(self._message)\n",
    "        return self\n",
    "        \n",
    "    def nltk_tweet_tokenizer(self):\n",
    "        self.tokens = tknz.TweetTokenizer().tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_with_regexp_tokenizer (self,regexp):\n",
    "        self.tokens = tknz.RegexpTokenizer(regexp).tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def nltk_sentence_tokenizer (self):\n",
    "        self.tokens = nltk.sent_tokenize(self._message)\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords_from_tokens (self,lang=None,is_new=True):\n",
    "        if lang is None:\n",
    "            sw = set(stopwords.words(\"english\"))\n",
    "        else :\n",
    "            sw = set(stopwords.words(lang))\n",
    "        tks = [token for token in self.tokens if token not in sw]\n",
    "        if is_new:\n",
    "            self.tokens_without_stops = tks\n",
    "        else :\n",
    "            self.tokens = tks\n",
    "        return self\n",
    "\n",
    "    def stemme_tokens (self,stemmer=None,is_new=True):\n",
    "        if stemmer is None:\n",
    "            stemmer = PorterStemmer()\n",
    "        xtr = [stemmer.stem(token) for token in self.tokens]\n",
    "        if is_new :\n",
    "            self.stem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self\n",
    "\n",
    "    def lematize_tokens (self,lematizer=None,is_new=True):\n",
    "        if lematizer is None:\n",
    "            lematizer = WordNetLemmatizer()\n",
    "        xtr = [lematizer.lemmatize(token) for token in self.tokens]\n",
    "        if is_new:\n",
    "            self.lem_tokens = xtr\n",
    "        else :\n",
    "            self.tokens = xtr\n",
    "        return self\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2312b9-ba2b-45f2-a962-c10e760fdc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "messages = []\n",
    "df1['tweet'].apply(lambda m: messages.append(m))\n",
    "df2['tweet'].apply(lambda m: messages.append(m))\n",
    "\n",
    "message_cleaner = MessageCleaner()\n",
    "\n",
    "new_messages=[]\n",
    "tokens=[]\n",
    "tokens_sw=[]\n",
    "tokens_stm=[]\n",
    "tokens_lm=[]\n",
    "\n",
    "for message in messages:\n",
    "    message_cleaner = MessageCleaner().set_message(message)\n",
    "    #message_cleaner.replace_by_regs()\n",
    "    message_cleaner.remove_users()\n",
    "    message_cleaner.remove_punctuation()\n",
    "    message_cleaner.remove_special_sym()\n",
    "    message_cleaner.remove_numbers()\n",
    "    message_cleaner.to_lower()\n",
    "    message_cleaner.replace_by_dicts(dictionary={\"can't\":\"can not\",\"ain't\": \"are not\"})\n",
    "    message_cleaner.replace_by_dicts(dictionary={\":)\":\"happy\"})\n",
    "    message_cleaner.escape_single_symbol_words()\n",
    "    message_cleaner.nltk_word_tokenize()\n",
    "    message_cleaner.remove_stopwords_from_tokens()\n",
    "    message_cleaner.stemme_tokens().lematize_tokens()\n",
    "    \n",
    "    new_messages.append(message_cleaner.get_message())\n",
    "    tokens.append(message_cleaner.tokens)\n",
    "    tokens_sw.append(message_cleaner.tokens_without_stops)\n",
    "    tokens_stm.append(message_cleaner.stem_tokens)\n",
    "    tokens_lm.append(message_cleaner.lem_tokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b84573-2326-4a40-bc64-d26b4421d18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "      <td>[studiolif, aislif, requir, passion, dedic, wi...</td>\n",
       "      <td>[studiolife, aislife, requires, passion, dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>[white, supremacists, want, everyone, to, see,...</td>\n",
       "      <td>[white, supremacists, want, everyone, see, new...</td>\n",
       "      <td>[white, supremacist, want, everyon, to, see, t...</td>\n",
       "      <td>[white, supremacist, want, everyone, to, see, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>[safe, ways, to, heal, your, acne, altwaystohe...</td>\n",
       "      <td>[safe, ways, heal, acne, altwaystoheal, health...</td>\n",
       "      <td>[safe, way, to, heal, your, acn, altwaystoh, h...</td>\n",
       "      <td>[safe, way, to, heal, your, acne, altwaystohea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "      <td>[hp, cursed, child, book, reservations, alread...</td>\n",
       "      <td>[is, the, hp, and, the, curs, child, book, up,...</td>\n",
       "      <td>[is, the, hp, and, the, cursed, child, book, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>[rd, bihday, to, my, amazing, hilarious, nephe...</td>\n",
       "      <td>[rd, bihday, amazing, hilarious, nephew, eli, ...</td>\n",
       "      <td>[rd, bihday, to, my, amaz, hilari, nephew, eli...</td>\n",
       "      <td>[rd, bihday, to, my, amazing, hilarious, nephe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "      <td>[ate, isz, that, youuu]</td>\n",
       "      <td>[ate, isz, youuu]</td>\n",
       "      <td>[ate, isz, that, youuu]</td>\n",
       "      <td>[ate, isz, that, youuu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>[to, see, nina, turner, on, the, airwaves, try...</td>\n",
       "      <td>[see, nina, turner, airwaves, trying, wrap, ma...</td>\n",
       "      <td>[to, see, nina, turner, on, the, airwav, tri, ...</td>\n",
       "      <td>[to, see, nina, turner, on, the, airwave, tryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>[listening, to, sad, songs, on, monday, mornin...</td>\n",
       "      <td>[listening, sad, songs, monday, morning, otw, ...</td>\n",
       "      <td>[listen, to, sad, song, on, monday, morn, otw,...</td>\n",
       "      <td>[listening, to, sad, song, on, monday, morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>[sikh, temple, vandalised, in, in, calgary, ws...</td>\n",
       "      <td>[sikh, temple, vandalised, calgary, wso, conde...</td>\n",
       "      <td>[sikh, templ, vandalis, in, in, calgari, wso, ...</td>\n",
       "      <td>[sikh, temple, vandalised, in, in, calgary, ws...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>thank you @user for you follow</td>\n",
       "      <td>[thank, you, for, you, follow]</td>\n",
       "      <td>[thank, follow]</td>\n",
       "      <td>[thank, you, for, you, follow]</td>\n",
       "      <td>[thank, you, for, you, follow]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "0      #studiolife #aislife #requires #passion #dedic...   \n",
       "1       @user #white #supremacists want everyone to s...   \n",
       "2      safe ways to heal your #acne!!    #altwaystohe...   \n",
       "3      is the hp and the cursed child book up for res...   \n",
       "4        3rd #bihday to my amazing, hilarious #nephew...   \n",
       "...                                                  ...   \n",
       "49154  ate @user isz that youuu?ðððððð...   \n",
       "49155    to see nina turner on the airwaves trying to...   \n",
       "49156  listening to sad songs on a monday morning otw...   \n",
       "49157  @user #sikh #temple vandalised in in #calgary,...   \n",
       "49158                   thank you @user for you follow     \n",
       "\n",
       "                                             tweet_token  \\\n",
       "0      [studiolife, aislife, requires, passion, dedic...   \n",
       "1      [white, supremacists, want, everyone, to, see,...   \n",
       "2      [safe, ways, to, heal, your, acne, altwaystohe...   \n",
       "3      [is, the, hp, and, the, cursed, child, book, u...   \n",
       "4      [rd, bihday, to, my, amazing, hilarious, nephe...   \n",
       "...                                                  ...   \n",
       "49154                            [ate, isz, that, youuu]   \n",
       "49155  [to, see, nina, turner, on, the, airwaves, try...   \n",
       "49156  [listening, to, sad, songs, on, monday, mornin...   \n",
       "49157  [sikh, temple, vandalised, in, in, calgary, ws...   \n",
       "49158                     [thank, you, for, you, follow]   \n",
       "\n",
       "                                    tweet_token_filtered  \\\n",
       "0      [studiolife, aislife, requires, passion, dedic...   \n",
       "1      [white, supremacists, want, everyone, see, new...   \n",
       "2      [safe, ways, heal, acne, altwaystoheal, health...   \n",
       "3      [hp, cursed, child, book, reservations, alread...   \n",
       "4      [rd, bihday, amazing, hilarious, nephew, eli, ...   \n",
       "...                                                  ...   \n",
       "49154                                  [ate, isz, youuu]   \n",
       "49155  [see, nina, turner, airwaves, trying, wrap, ma...   \n",
       "49156  [listening, sad, songs, monday, morning, otw, ...   \n",
       "49157  [sikh, temple, vandalised, calgary, wso, conde...   \n",
       "49158                                    [thank, follow]   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "0      [studiolif, aislif, requir, passion, dedic, wi...   \n",
       "1      [white, supremacist, want, everyon, to, see, t...   \n",
       "2      [safe, way, to, heal, your, acn, altwaystoh, h...   \n",
       "3      [is, the, hp, and, the, curs, child, book, up,...   \n",
       "4      [rd, bihday, to, my, amaz, hilari, nephew, eli...   \n",
       "...                                                  ...   \n",
       "49154                            [ate, isz, that, youuu]   \n",
       "49155  [to, see, nina, turner, on, the, airwav, tri, ...   \n",
       "49156  [listen, to, sad, song, on, monday, morn, otw,...   \n",
       "49157  [sikh, templ, vandalis, in, in, calgari, wso, ...   \n",
       "49158                     [thank, you, for, you, follow]   \n",
       "\n",
       "                                        tweet_lemmatized  \n",
       "0      [studiolife, aislife, requires, passion, dedic...  \n",
       "1      [white, supremacist, want, everyone, to, see, ...  \n",
       "2      [safe, way, to, heal, your, acne, altwaystohea...  \n",
       "3      [is, the, hp, and, the, cursed, child, book, u...  \n",
       "4      [rd, bihday, to, my, amazing, hilarious, nephe...  \n",
       "...                                                  ...  \n",
       "49154                            [ate, isz, that, youuu]  \n",
       "49155  [to, see, nina, turner, on, the, airwave, tryi...  \n",
       "49156  [listening, to, sad, song, on, monday, morning...  \n",
       "49157  [sikh, temple, vandalised, in, in, calgary, ws...  \n",
       "49158                     [thank, you, for, you, follow]  \n",
       "\n",
       "[49159 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'tweet': messages,'tweet_token': tokens,'tweet_token_filtered': tokens_sw, 'tweet_stemmed': tokens_stm, 'tweet_lemmatized': tokens_lm} \n",
    "df = pd.DataFrame(dict)\n",
    "df.to_pickle(\"xxxx.pickle\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09dc08-0257-400e-8443-0bf577d77c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435658d-0ceb-498b-a860-b1d3e47b3afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396d23d-6334-48ae-b8fd-2438440dc2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2848b43-901a-4f6a-968c-ac47f12292b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb34c1-65d1-4144-8609-3f6902ef83c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
