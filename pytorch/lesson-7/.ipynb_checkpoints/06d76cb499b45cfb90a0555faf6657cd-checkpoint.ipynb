{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6_vcBAmo6k9"
   },
   "source": [
    "# Рекурентные сети для обработки последовательностей\n",
    "\n",
    "Вспомним все, что мы уже знаем про обработку текстов:\n",
    "- Компьютер не понимает текст, поэтому нам нужно его как-то закодировать - представить в виде вектора\n",
    "- В тексте много повторяющихся слов/лишний слов - нужно сделать препроцессинг:\n",
    "    - удалить знаки препинания\n",
    "    - удалить стоп-слова\n",
    "    - привести слова к начальной форме (**стемминг** и **лемматизация**)\n",
    "    - ???\n",
    "    \n",
    "    \n",
    "- После этого мы можем представить наш текст (набор слов) в виде вектора, например, стандартными способами:\n",
    "    - **CounterEncoding** - вектор длины размер нашего словаря\n",
    "        - есть словарь vocab, который можем включать слова, ngram-ы\n",
    "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=1,\\ если\\ vocab[i]\\ \\in\\ doc$\n",
    "    - **HashingVectorizer** - вектор заранее заданной длины\n",
    "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=1,\\ если\\ \\exists\\ txt\\ \\in\\ doc:\\ hash(text)\\ =\\ i$\n",
    "    - **TfIdfVectorizer** - вектор длины размер нашего словаря\n",
    "        - есть словарь vocab, который можем включать слова, ngram-ы\n",
    "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=tf(vocab[i])*idf(vocab[i]),\\ если\\ vocab[i]\\ \\in\\ doc$\n",
    "    \n",
    "        $$ tf(t,\\ d)\\ =\\ \\frac{n_t}{\\sum_kn_k} $$\n",
    "        $$ idf(t,\\ D)\\ =\\ \\log\\frac{|D|}{|\\{d_i\\ \\in\\ D|t\\ \\in\\ D\\}|} $$\n",
    "        \n",
    ", где \n",
    "- $n_t$ - число вхождений слова $t$ в документ, а в знаменателе — общее число слов в данном документе\n",
    "- $|D|$ — число документов в коллекции;\n",
    "- $|\\{d_i\\ \\in\\ D\\mid\\ t\\in d_i\\}|$— число документов из коллекции $D$, в которых встречается $t$ (когда $n_t\\ \\neq\\ 0$).\n",
    "\n",
    "\n",
    "\n",
    "Это база и она работает. Мы изучили более продвинутые подходы: эмбединги и сверточные сети по эмбедингам. Но тут есть проблема: любой текст - это последовательность, ни эмбединги, ни сверточные сети не работают с ним как с последовательностью. Так давайте попробуем придумать архитектуру, которая будет работать с текстом как с последовательностью, двигаясь по эмбедингам и как-то меняя их значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5RrHMlfo6k_"
   },
   "source": [
    "## Придумаем сами архитектуру, чтобы работать с последовательностями\n",
    "\n",
    "Возьмем перцептрон с входом - эмбедингом слова (пусть пока он фиксированный) и будем пытаться классифицировать каждое слово.\n",
    "\n",
    "Почему классифицировать? потому что это частая задача в обработке языка + это дает возможность генерировать текст (просто классифицируем на кол-во классов = кол-ву слов в словаре).\n",
    "\n",
    "<img src=\"images/Single_layer_perceptron.png\">\n",
    "\n",
    "Какая тут последовательность? никакой, но давайте на вход подавать эмбединг, но в 1 скрытый слой будем добавлять последний скрытый слой предыдущего шага)\n",
    "\n",
    "<img src=\"images/Rnnbr.png\">\n",
    "\n",
    "То есть мы прокидываем информацию с предыдущего шага, а за счет того, что мы все время так стекаем вектора мы получаем то, что информация проходит через текст от начала до конца. Что делать с 1 шагом? -> Добавим вектор из нулей. И вот мы получили первую рекурентную сеть. Чаще её рисуют следующим образом:\n",
    "\n",
    "\n",
    "<img src=\"images/Rnn.png\">\n",
    "\n",
    "Итак, мы придумали простую рекуретную сеть. Последний открытый вопрос как её обучать?\n",
    "\n",
    "Все также, градиентным спуском, нам нужно двигаться во времени и обновлять параметры, поэтому обучение таких сетей занимает очень много времени (вы не можете обновить веса для 1-го токена, пока не посчитаете градиент сквозь время).\n",
    "\n",
    "Мы тренируем такую систему, где промежуточный выход с сети прошлого шага дают на вход сети на следующем шаге. Это значит, что она может выучить некоторое  представление входа. Как-то его закодировать в вектор и передать самой себе на следующем шаге, чтобы знать, что происходило раньше. И таким образом результирующий выход сети, он уже зависит от всей накопленной последовательности. И это дает нам возможность на основе всей последовательности выдать выход.  \n",
    "И тот факт, что мы даем на вход сети самой себе - это некоторая рекурсия и поэтому такая архитектура называется так, как она называется.\n",
    "\n",
    "## 1.2 Давайте посмотрим как это примерно выглядит в коде:\n",
    "\n",
    "class RNN:\n",
    "\n",
    "  # ...\n",
    "  \n",
    "  def step(self, x):\n",
    "  \n",
    "    # update the hidden state\n",
    "    \n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    \n",
    "    # compute the output vector\n",
    "    \n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "Вот наш шаг - т.е. forward, прямой проход. Он выглядит следующим образом:  \n",
    "Мы делаем матричное произведение весов W и текущего состояния h и добавляем тоже произведение весов W на вход x. и получаем новый h.\n",
    "\n",
    "\n",
    "У нас есть набор X, где t - элементы последовательности. Мы сливаем h и X_t вместе, прогоняем через слой, через тангенсальную функцию активации. и получаем следующие h.\n",
    "\n",
    "Почему tanh? А например не ReLU? Второй всегда отрезает отрицательную часть сигнала. И это имеет такое последствие, что сигнал, который проходит через сеть никогда не сможет стать отрицательным. И это значит, что когда через много слоев проходим - то сигнал может только расти. Поэтому пользуются именно tanh, что бы были возможности получить и плюс, и минус.\n",
    "\n",
    "## 1.3 Количество слоев\n",
    "\n",
    "Можно так же настэкать большое количество слоев, как и везде.\n",
    "Вот так выглядит рекурентная сеть с несколькими слоями:\n",
    "\n",
    "<img src=\"images/layer.png\">\n",
    "<img src=\"images/layer.png\">\n",
    "\n",
    "То есть обычным образом можно прогнать сигнал через слои сети. И при этом каждый из слоев выдает своему эквивалентному слою в следующей итерации сети некоторое скрытое состояние. Чем больше слоев, тем сеть обладает большей обобщающей способностью. И каждый слой на своем уровне понимания может себе в будущем передать состояние. При этом веса у каждого эквивалентного слоя одни и те же.\n",
    "\n",
    "\n",
    "## 1.4 Варианты связей\n",
    "\n",
    "Что делать, если мы хотим классифицировать текст целиком? оставить только последний выход!\n",
    "\n",
    "<img src=\"images/RnnTasks.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "1) Мы познакомимся с архитектурой, которая может на вход получить один вход, а на выходе иметь несколько.   \n",
    "2) И наоборот, на вход подать несколько (причем неизвестного количества), а на выходе одно значение. Один из примеров такой задачи - это [sentimental analysis](https://monkeylearn.com/sentiment-analysis/). Т.е. на вход идет некое предложение, а на выход мы хотим получить одно значение. Например, оно позитивное или негативное или еще какое.  \n",
    "3,4) Или же могут быть варианты многие ко многим. На вход подаем некоторое переменное количество параметров, а на выходе другое тоже переменое количество параметров.  Пример такой задачи - это перевод текста (перевод на другие иностранные языки). Или другой вариант многие ко многим - это когда количество параметров на входе фиксированно и соотвествует выходу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 реализация\n",
    "\n",
    "\n",
    "Давайте посмотрим как это примерно выглядит в коде для прошлого примера\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BpooDtBoo6lB"
   },
   "outputs": [],
   "source": [
    "# попробуем запрограммировать простую рекурентную сеть. Возьмем датасет с прошлого занятия\n",
    "\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_val = pd.read_csv(\"data/val.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v7G2jv7Ro6lC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  class\n",
       "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dfMrjVd6o6lD"
   },
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xrIVhJYlo6lD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V2XxVPWao6lE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 258107\n",
      "num_words after: 66106\n"
     ]
    }
   ],
   "source": [
    "text_corpus_train = df_train['text'].values\n",
    "text_corpus_valid = df_val['text'].values\n",
    "text_corpus_test = df_test['text'].values\n",
    "\n",
    "counts = Counter()\n",
    "for sequence in text_corpus_train:\n",
    "    counts.update(sequence.split())\n",
    "\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))\n",
    "    \n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q6ClV7nio6lE"
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, txts, labels, w2index, used_length):\n",
    "        self._txts = txts\n",
    "        self._labels = labels\n",
    "        self._length = used_length\n",
    "        self._w2index = w2index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._txts)\n",
    "    \n",
    "    @lru_cache(50000)\n",
    "    def encode_sentence(self, txt):\n",
    "        encoded = np.zeros(self._length, dtype=int)\n",
    "        enc1 = np.array([self._w2index.get(word, self._w2index[\"UNK\"]) for word in txt.split()])\n",
    "        length = min(self._length, len(enc1))\n",
    "        encoded[:length] = enc1[:length]\n",
    "        return encoded, length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded, length = self.encode_sentence(self._txts[index])\n",
    "        return torch.from_numpy(encoded.astype(np.int32)), self._labels[index], length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_xs-Ka_co6lF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(i.split()) for i in text_corpus_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dxSAuc_Co6lG"
   },
   "outputs": [],
   "source": [
    "y_train = df_train['class'].values\n",
    "y_val = df_val['class'].values\n",
    "\n",
    "train_dataset = TwitterDataset(text_corpus_train, y_train, vocab2index, 27)\n",
    "valid_dataset = TwitterDataset(text_corpus_valid, y_val, vocab2index, 27)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=3)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "документация : https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O38bou70o6lG"
   },
   "outputs": [],
   "source": [
    "class RNNFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        rnn_out, (ht, ct) = self.rnn(x)\n",
    "        return self.linear(rnn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0XE-LnMho6lH"
   },
   "outputs": [],
   "source": [
    "rnn_init = RNNFixedLen(len(vocab2index), 30, 20)\n",
    "optimizer = torch.optim.Adam(rnn_init.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w7zupF56o6lH"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KWlo5hTOo6lI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_832488/3822812957.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(10)):\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0030961036682128906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fe82897001405eb4d108b442e9f114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 valid_loss 121.86559295654297\n",
      "Epoch 1 valid_loss 118.90132904052734\n",
      "Epoch 2 valid_loss 117.23704528808594\n",
      "Epoch 3 valid_loss 116.75323486328125\n",
      "Epoch 4 valid_loss 115.91516876220703\n",
      "Epoch 5 valid_loss 115.1673355102539\n",
      "Epoch 6 valid_loss 114.79803466796875\n",
      "Epoch 7 valid_loss 114.02484130859375\n",
      "Epoch 8 valid_loss 114.800048828125\n",
      "Epoch 9 valid_loss 116.95244598388672\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(10)):  \n",
    "    rnn_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = rnn_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    rnn_init.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long()\n",
    "        y = y.long().view(-1, 1)\n",
    "        output = rnn_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m91E6Wo7o6lI"
   },
   "source": [
    "# Какие проблемы у рекурентных сетей?\n",
    "\n",
    "\n",
    "Вот у нас есть рекуррентная сеть. Она дает себе на вход значение на следующем шаге. Когда мы ее тренируем, мы как бы разматываем всю эту систему в одну большую сеть, которая прогоняет все элементы последовательности. И у всех слоев сетей в разные промежутки времени одни и те же веса. Соответсвенно, проходясь обратным распространением по этим сетям все это вместе складываем и применяем. Так происходит обучение.  \n",
    "\n",
    "И от этого возникает проблема длинных зависимостей. Т.е. например, то что произошло в начале последовательности, может повлиять на то, что произойдет в конце этой последовательности. Для этого нужно, что бы сигнал во время тренировки протек по длинному пути всей последовательности. А у нас тут очень много матричных умножений на одну и ту же матрицу. Например, у нас 100 таких шагов. Что бы градиент прошел обратно, он будет сто раз умножен на одну и ту же матрицу. И это критично. Т.к. если, например, эта матрица какой-то один сигнал увеличивает чуть. То после того, как оно пройдет 100 раз, сеть этот сигнал сделает огромным. А у нас там tanh, который этот сигнал совсем убьет, т.к. сделает его очень большшим. \n",
    "\n",
    "Поэтому в такой простой формулировке, большие последовательности не получается тренировать. И на практике используют другие архитектуры, которые эту проблему решают.\n",
    "\n",
    "Резюме:\n",
    "\n",
    "- затухают градиенты\n",
    "- медленно, нужно всегда дойти до конца\n",
    "\n",
    "Как решить? -> LSTM\n",
    "\n",
    "[оригинальная статья. 1997](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)\n",
    "\n",
    "Автор статьи [Юрген Шмидхубер](https://ru.wikipedia.org/wiki/%D0%A8%D0%BC%D0%B8%D0%B4%D1%85%D1%83%D0%B1%D0%B5%D1%80,_%D0%AE%D1%80%D0%B3%D0%B5%D0%BD). Первая реализации ее случилась только после несколько лет после ее публикации.\n",
    "\n",
    "[статья](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), которая объясняет что там происходит\n",
    "\n",
    "[перевод](https://alexsosn.github.io/ml/2015/11/17/LSTM.html) статьи выше\n",
    "\n",
    "<img src=\"images/lstm.png\">\n",
    "\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "\n",
    "Давайте, кратко посмотрим как это работает:\n",
    "\n",
    "\n",
    "<img src=\"images/LSTMMaths.png\">\n",
    "\n",
    "\n",
    "\n",
    "Основная идея.  \n",
    "\n",
    "Теперь на каждом шаге мы передаем не один вектор, а два. Один из них (нижний) - это то самый h, который пойдет на вход следующему слою. А кроме этого, мы будем передавать, так называемое self-state. Назовем этот вектор - с. И вот h будет теперь проходить через много нелинейностей, умножаться на матрицы и будет испытывать все те же проблемы, которые мы обсуждали выше.  \n",
    "\n",
    "А с будет как можно больше напрямую передаваться из прошлого состояния в следующее. Мы по нему будем очень аккуратно делать апдейты, что бы он очень плавно перетекал из одного состояния в другое.  \n",
    "\n",
    "### 2.1 Как это выглядит подробнее:  \n",
    "\n",
    "Внутри есть мнгого разных гейтов - некоторый вектор коэффициентов, которые соответсвуют той же размерности что и вход на них.  \n",
    "\n",
    "\n",
    "#### **Forget gate**\n",
    "\n",
    "Первый шаг в LSTM - это решить, от какой информации мы хотим избавиться. Это решение принимает слой с сигмоидой, который называется \"forget gate layer.\" (гейт забывания). Он принимает во внимание $h_{t−1}$ и $x_t$, а на выходе даёт значение между 0 и 1 для каждого числа в состоянии ячейки $C_{t−1}$. 1 значит \"полностью сохрани это\", а 0 - \"полностью забудь это\".\n",
    "\n",
    "Пример забывания - языковая модель пытается предсказать следующее слово базируясь на предыдущих. Здесь модель может запоминать род объекта, чтобы использовать правильное образование слов. Когда мы видим новый объект, то нужно забыть род предыдущего объекта. *(Животное не переходило дорогу, потому что оно устало)*\n",
    "\n",
    "#### **Input gate**\n",
    "\n",
    "Следующий шаг - решить, какую информацию мы должны хранить в состоянии ячейки. Шаг состоит из двух частей. Первая - слой сигмоиды, называемый \"input gate layer\" (входной гейт), который решает какие значения будут обновляться. Вторая - слой с тангенсом, который создает вектор значений $\\tilde{C}_t$, которые будут добавляться к состоянию ячейки. Используем tanh, потому что нам хочется что бы эта добавка могла пойти и в + и в -.\n",
    "\n",
    "\n",
    "С примером языковой модели, мы бы хотели добавлять род нового объекта в состояние ячейки, чтобы заменить старый род, который мы забудем. *(Животное не переходило дорогу, потому что оно устало)*\n",
    "\n",
    "#### **Update cell state**\n",
    "\n",
    "Сейчас самое время, чтобы обновить старое состояние $C_{t−1}$ в новое состояние $C_t$. Предыдущие шаги уже решили, что делать, нужно только сделать это.\n",
    "\n",
    "Умножаем старое состояние на $f_t$, тем самым забывая те вещи, которые хотели забыть, затем прибавляем $i_t∗\\tilde{C_t}$. Это новое значение состояния ячейки, которое отмасштабировано в зависимоcти от того, насколько мы хотим обновить новое значение.\n",
    "\n",
    "В языковой модели, это момент, где мы выкидываем информацию о роде старого объекта и добавляем новую информацию о роде нового объекта. *(Животное не переходило дорогу, потому что оно устало)*\n",
    "\n",
    "#### **Output gate**\n",
    "\n",
    "Наконец-то нам нужно решить, что мы отправим на выход. Выход будет базироваться на состоянии ячейки, но с небольшой фильтрацией. Во-первых, прогоним входной сигнал через сигмоиду, которая решает с какой силой дальше пропускать сигнал, во-вторых, прогоняем состояние ячейки через тангенс и умножаем это на сигмоиду, чтобы пропускать дальше только то, что мы решили пропустить.\n",
    "\n",
    "\n",
    "Для языковой модели, которая видит только объект, здесь можем пропустить информацию, связанную с глаголом. Например, на выходе может быть полезно число множественной или единственное у объекта, чтобы знать в какую форму нужно поставить глагол. *(Животное не переходило дорогу, потому что оно устало)*\n",
    "\n",
    "Основная идея зачем мы все это делаем - это бы мы на следующий шаг передавали два вектора. Один из них h в процессе своего формирования прошел через огромное количество нелинейностей, одних и тех же весов и т.д. и по нему градиент идет ни хорошо, ни плохо.  \n",
    "\n",
    "если все это сложить прошлое со следующим. То видим что мы организовали такой некий highway на котором градиент меньше всего затухает.\n",
    "\n",
    "документация : https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rnn_init.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dzlCNORAo6lJ"
   },
   "outputs": [],
   "source": [
    "class LSTMFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(lstm_out)\n",
    "    \n",
    "lstm_init = LSTMFixedLen(len(vocab2index), 10, 20).to(device)\n",
    "optimizer = torch.optim.Adam(lstm_init.parameters(), lr=0.00051)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 27, 1]), torch.Size([128, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qw1Mpj_Co6lJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_832488/3832155387.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(10)):\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0056819915771484375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065951f582564c418938d1a9de859145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     pred_test_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze((output \u001b[38;5;241m>\u001b[39m th)\u001b[38;5;241m.\u001b[39mint())\n\u001b[1;32m     29\u001b[0m     test_running_right \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y \u001b[38;5;241m==\u001b[39m pred_test_labels)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 31\u001b[0m test_loss_history\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtest_loss\u001b[49m\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Test acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_running_right\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtest_running_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m valid_loss \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, loss_accumed  \u001b[38;5;241m/\u001b[39m test_running_total))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "th = 0.5\n",
    "test_loss_history = []\n",
    "for epoch in tqdm_notebook(range(10)):  \n",
    "    lstm_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long().to(device)\n",
    "        labels = labels.long().view(-1, 1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = lstm_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    lstm_init.eval()\n",
    "    loss_accumed = 0\n",
    "    test_running_total = 0\n",
    "    test_running_right = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long().to(device)\n",
    "        y = y.long().view(-1, 1).to(device)\n",
    "        output = lstm_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        test_running_total += 1\n",
    "        loss_accumed += loss\n",
    "        pred_test_labels = torch.squeeze((output > th).int())\n",
    "        test_running_right += (y == pred_test_labels).sum()\n",
    "\n",
    "        test_loss_history.append(loss.item())\n",
    "    print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed  / test_running_total))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2gbd_Oao6lK"
   },
   "source": [
    "# Какие проблемы:\n",
    "\n",
    "\n",
    "\n",
    "Резюме:\n",
    "\n",
    "- вычислительно сложно -> медленнее\n",
    "- на очень длинных последовательностях все равно затухает градиент\n",
    "\n",
    "\n",
    "Зачем платить больше - уберем некоторые врата (точнее совместим) -> ускоримся, уменьшим число параметров -> GRU\n",
    "\n",
    "\n",
    "<img src=\"images/gru.png\">\n",
    "\n",
    "\n",
    "GRU Math\n",
    "\n",
    "\n",
    "<img src=\"images/GRUMath.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jtq6UR4co6lK"
   },
   "outputs": [],
   "source": [
    "class GRUFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        gru_out, (ht, ct) = self.gru(x)\n",
    "        return self.linear(gru_out)\n",
    "    \n",
    "gru_init = GRUFixedLen(len(vocab2index), 30, 20)\n",
    "optimizer = torch.optim.Adam(gru_init.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7_5BDD5Go6lK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_832488/3529531258.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(10)):\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0031495094299316406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24bfcab84ba422682932771bf31b700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 valid_loss 120.7620849609375\n",
      "Epoch 1 valid_loss 118.59281158447266\n",
      "Epoch 2 valid_loss 116.7489242553711\n",
      "Epoch 3 valid_loss 117.6670913696289\n",
      "Epoch 4 valid_loss 114.25006103515625\n",
      "Epoch 5 valid_loss 114.1899185180664\n",
      "Epoch 6 valid_loss 114.4083023071289\n",
      "Epoch 7 valid_loss 115.05182647705078\n",
      "Epoch 8 valid_loss 115.34483337402344\n",
      "Epoch 9 valid_loss 115.6536865234375\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(10)):  \n",
    "    gru_init.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, lengths = data[0], data[1], data[2]\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.long().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = gru_init(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    gru_init.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y, lengths in valid_loader:\n",
    "        X = X.long()\n",
    "        y = y.long().view(-1, 1)\n",
    "        output = gru_init(X, lengths)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siseT32bo6lL"
   },
   "source": [
    "3 подхода:\n",
    "\n",
    "<img src=\"images/RNNCompar.png\">\n",
    "\n",
    "\n",
    "Как регуляризовать?\n",
    "- дропаут\n",
    "- рекурентный дропаут\n",
    "\n",
    "\n",
    "<img src=\"images/Dropouts.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "24YgtqKQo6lL"
   },
   "outputs": [],
   "source": [
    "# Можно строить lstm с переменным размером входа:\n",
    "class LSTM_variable_input(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "1. Попробуйте обучить нейронную сеть GRU/LSTM для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "\n",
    "2. Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?\n",
    "\n",
    "У кого нет возможности работать через каггл (нет верификации), то можете данные взять по ссылке: https://drive.google.com/file/d/1czQcI0Zgvgo6DjW1-yTFUhL8_XVsF6vi/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "А. Зачем испльзуют рекуррентные сети:\n",
    "\n",
    "  1. для сокращения времени обучения\n",
    "  2. для борьбы с исчезновением градиента\n",
    "  3. для использования контекста\n",
    "\n",
    "В. В чем проблема \"ванильных\"  рекуррентных сетей:\n",
    "\n",
    "  1. большое время для обучения\n",
    "  2. Исчезновение градиента\n",
    "  3. отсутствие методов обучения\n",
    "\n",
    "С. Какую проблему решает LSTM:\n",
    "\n",
    "  1. сокращение времени обучения\n",
    "  2. борьба с исчезновением градиента\n",
    "  3. использование контекста\n",
    "\n",
    "D. Какую проблему решает GRU:\n",
    "\n",
    "  1. сокращение времени обучения\n",
    "  2. борьба с исчезновением градиента\n",
    "  3. использование контекста\n",
    "\n",
    "Е. Какие узлы позволяют решать более точно задачи (в среднем):\n",
    "  1. LSTM\n",
    "  2. RNN\n",
    "  3. GRU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
