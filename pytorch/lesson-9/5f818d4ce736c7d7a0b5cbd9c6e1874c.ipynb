{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "\n",
    "class LSTMFixedLen(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(lstm_out)\n",
    "    \n",
    "lstm_init = LSTMFixedLen(vocab_size, embedding_dim, hidden_dim)\n",
    "optimizer = torch.optim.Adam(lstm_init.parameters(), lr=0.00051)\n",
    "#criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.BCELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class Data(Dataset):\n",
    "    def __init__(self, func, min, max, num):\n",
    "        self.num = num\n",
    "        self.x = np.random.rand(num) * (max - min) + min\n",
    "        self.func = func\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point = self.x[idx]\n",
    "        # return torch.FloatTensor([point, self.func(point)])\n",
    "        return torch.Tensor([point, self.func(point)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.linear_2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.linear_2(x)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcJ1AOYlkdqQ"
   },
   "source": [
    "# В предыдущей серии\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=12ZNQ57Fj9arjbwRR-jS6atV8o_IXTMwI\"/>\n",
    "\n",
    "\n",
    "Мы посмотрели на задачу классификации текстов. Но есть ряд более сильных подходов, которые лучше показывать через задачу генерации\n",
    "\n",
    "\n",
    "# Генерация текстов, encoder-decoder\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vq571MgxcLaKXyE5E0-qxpauwIZmSc1T\"/>\n",
    "\n",
    "\n",
    "Данная архитектура называется seq2seq, простыми словами выглядит она следующим образом:\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1MAi5s0Jl0QdvOFU_ILJqQLk5stsPLHYF\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VibWuP0jGt4F"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLDjJ2kkH3iK"
   },
   "source": [
    "Примеры из статьи [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144).\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1uOHQX8AnPGKY6HEDOTRJzVxCiKAHLoJy' width=450>\n",
    "\n",
    "Нужно сжать всю информацию о предыдущих словах в один блок, который представляет собой внутренний слой - переходное состояние из кодировщика в декодировщика, что очень сложная задача, которая приводит к недообучению.\n",
    "\n",
    "И одно из возможных решений - это настакать слои LSTM, один слой LSTM создает вход для другого слоя LSTM:\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1yrBxb5DXZA-LNwNWQdmuF4TNNXqTfNpJ' width=450>\n",
    "\n",
    "Но эту вещь очень тяжело обучать, если добавлять всё больше LSTM слоем, то снова встречаемся с проблемой затухающих градиентов, плюсом, всё равно нужно сжимать всю информацию в последние блоки - это наш bottleneck (узкое место, горлышко бутылки) между кодировщик и декодировщиком.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY_szeB8KpkH"
   },
   "source": [
    "И решение этой проблемы - Attention слой. Вместо того, чтобы сжимать всю информацию из всех временных ячеек в одно скрытое состояние, можно дать доступ декодировщику ко всей истории. Но получаем очень слишком много информации, которую нужно учитывать, поэтому мы будем обращать свое внимание только на подвыборку этих ячеек.\n",
    "\n",
    "Будем учитывать, какая часть предложения на английском языке важна для предсказания слова на французском языке. Подсчет такого распределения достигается получением оценки релевантности каждого слова в предложении для получения нового слова.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1GAVWyShMSXELaz1f-_e4c9rAeDoJ3QE_' width=450>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CKXDdDGOTB2"
   },
   "source": [
    "**Как использовать эти оценки релевантности?**\n",
    "\n",
    "Подсчитываются оценки для каждого скрытого состояния и образуют собой взвешанную сумму внимания. А потом эта сумма передается, как еще один вход в декодировщик.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1a72Ake6Ai2SdkZ9cQpqmq6-ebjblcQ34' width=450>\n",
    "\n",
    "Можно [провизуализировать](https://distill.pub/2016/augmented-rnns/#attentional-interfaces), какое внимание уделяется каждому слову из предложения:\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1OJ4vgTHPxGMzcR8Ob8An--FFUlnZabQ3' width=500>\n",
    "\n",
    "А ещё слои внимания можно добавлять и не только к текстам, но и к картинкам.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1i48NmZMXUneeUwW339tSgvrGU_VvLh9H'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbmWL935kdqw"
   },
   "source": [
    "Теперь более детально, как рассчитать внимание.\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=12oHuf30zRZR_rEQ3P7Y599qZVexH50C1\"/>\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1XOmhFw2RvweVYl9m7pUN0QQD5R0Kj_yl\"/>\n",
    "</td>\n",
    "</td>\n",
    "</table>\n",
    "\n",
    "\n",
    "- h(t): скрытое состояние декодера\n",
    "- c(t): вектор контекста, который подается на вход\n",
    "- y(t): текущий таргет\n",
    "- $\\bar{h}(t)$: скрытое состояние attention\n",
    "- a(t): скор нормализации\n",
    "\n",
    "\n",
    "Зачем нужен скор нормализации? - пытаемся сравнить похожесть текущего скрытого состояния и скрытого состояния из прошлого и понять, на что обращать внимание\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4A8NwtvgJdNV"
   },
   "source": [
    "#### Attention Decoder\n",
    "\n",
    "Туториал: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "Но если передается только один контекстный вектор с энкодера в декодер, то в нём должна быть вся информация, что естественно очень тяжело выучить.\n",
    "\n",
    "Attention позволяет декодеру сфокусироваться на разных частях выхода энкодера. Сначала считаются веса attention'a. Они перемножаются на выходы энкодера, чтобы создать взвешенную комбинацию. \n",
    "\n",
    "Вычисление весов внимание можно сделать через полносвязную сеть, используя входы декодера и скрытые состояния.\n",
    "\n",
    "Из-за того, что предложения разной длины, невозможно обучать такой слой, так что нужно выбрать максимальную длину предложения входной последовательности и так обучаться. Предложения максимальной длины будут использовать все веса внимания, а короткие предложения только часть.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1Wde98PnXvXEw9jHBca524pEy45yBfQzd'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a_XdDKS7JfdH"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input - один вход на декодер (сымый первый - это токен SOS)\n",
    "        embedded = self.embedding(input).view(1, -1)  # (1, hidden_size)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded, hidden), 1)),\n",
    "            dim=1)  # (1, max_length)\n",
    "\n",
    "        # encoder_outputs (max_length, hidden_size)\n",
    "        attn_applied = torch.matmul(attn_weights, encoder_outputs)  # (1, hidden_size)\n",
    "\n",
    "        output = torch.cat((embedded, attn_applied), 1) # (1, hidden_size*2)\n",
    "        output = self.attn_combine(output)  # (1, hidden_size)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output.unsqueeze(0), hidden)  # (1, 1, hidden_size)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)  # (1, vocab_size)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FKkZy9Hwb6M"
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZjZccJw61at"
   },
   "source": [
    "### Общее представление"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wnjWLTXwd3y"
   },
   "source": [
    "[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "\n",
    "В трансформере находится энкодер и декодер, которые в свою очередь представляют стэк энкодеров и декодеров (в оригинальной статье используется по 6 каждого).   \n",
    "\n",
    "<img src='https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png' width=500>\n",
    "\n",
    "Энкодеры все идентичны по структуре, хотя у них разные веса. Каждый можно поделить на два подслоя:\n",
    "\n",
    "<img src='https://jalammar.github.io/images/t/Transformer_decoder.png' width=500>\n",
    "\n",
    "Вход энкодера сначала поступает на слой self-attention'a - слой, который позволяет энкодеру смотреть на другие слова во входной последовательности, чтобы более качественно закодировать слово.  \n",
    "Выходы из self-attention'a скармливаются в сетку с прямым проходом.\n",
    "\n",
    "У декодера есть такие же два слоя (self-attention и feed forward сетка), но между ними добавляется слой внимания, который фокусируется на релевантных частях входной последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rsuagsoh7Foo"
   },
   "source": [
    "### Устройство энкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgK6tV_Zz7rQ"
   },
   "source": [
    "Теперь добавим ещё больше деталей. В самом начале нужно перевести входные слова в эмбеддинги. Каждое слово будет представлять из себя вектор размерности 512.  \n",
    "Эмбеддинги передаются только в самом первом энкодере, дальше передаются уже выходы из промежуточных энкодеров. Размер таких последовательностей - это гиперпараметр, который можем установить размером самого длинного предложения в обучающей выборке.\n",
    "\n",
    "\n",
    "<img src='https://jalammar.github.io/images/t/encoder_with_tensors.png' width=500>\n",
    "\n",
    "Начинает проглядываться ключевая особенность трансформера - слово в последовательности протекает по своему собственному пути в энкодере. Есть зависимости между этими дорожками в слое self-attention'a, а вот в feed-forward слое таких зависимостей нет, поэтому такие вычисления могут совершаться параллельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-CjJLsP54Nq"
   },
   "source": [
    "\n",
    "Первый шаг в вычислении self-attention - создание трех векторов из каждого входного эмбеддинга. Для каждого слова создается вектор Query, вектор Key и вектор Value. Эти векторы создаются путем умножения эмбеддинга на три матрицы, которые обучаются.\n",
    "- Query - слово, с которого смотрим на всё остальное\n",
    "- Key - слово, на которое смотрим\n",
    "- Value - здесь содержится смысл слова\n",
    "\n",
    "При перемножении Query на Key мы понимаем насколько слово с Key релевантно слову с Query.\n",
    "\n",
    "<img src='http://jalammar.github.io/images/t/transformer_self_attention_vectors.png' width=500>\n",
    "\n",
    "Все шаги вместе:\n",
    "\n",
    "<img src='https://jalammar.github.io/images/t/self-attention-output.png' width=500>\n",
    "\n",
    "Деление на $\\sqrt{d}$ (квадратный корень из размерности Key векторов. для того, чтобы были более стабильные градиенты.\n",
    "\n",
    "<img src='http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mo81F5N7LDe"
   },
   "source": [
    "### Multi-head self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otvtmRGM57hA"
   },
   "source": [
    "\n",
    "<img src='http://jalammar.github.io/images/t/transformer_attention_heads_qkv.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk6ELZFm1yLh"
   },
   "source": [
    "В трансформерах используется не один слой self-attention'a, а несколько, так называемый механизм multi-headed attention. Это улучшает обычный слой внимания двумя путями:\n",
    "1. Он расширяет возможность модели фокусироваться на разных позициях. Иногда внимание может упасть на само это слово, а на другие не будет обращать внимание. (*Животное не переходило дорогу, потому что оно устало*)\n",
    "\n",
    "2. Он позволяет слою внимания иметь несколько признаковых описаний. Раз голов внимания больше, то и будет больше наборов с матрицами весов, которые как-то переводят наши слова в векторы для энкодера/декодера. Получаем, что после обучения, каждый такой набор матриц позволит переводить входной эмбеддинг в разные пространства.\n",
    "\n",
    "<img src='https://jalammar.github.io/images/t/transformer_attention_heads_z.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ9cqBsX3jOe"
   },
   "source": [
    "В примере 8 голов слоя внимания, но feed forward сеть не ожидает столько входов, поэтому нужно сконкатенировать выходы, перемножить на матрицу весов и получить вектор, который является собирательным описанием всех голов внимания. <img src='https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H198yU1U4E3f"
   },
   "source": [
    "Еще одна деталь в трансформерах:  \n",
    "они добавляют позиционные векторы ко входным эмбеддингам, эти векторы следуют паттерны, которые изучает модель, чтобы помогать определять позиции каждого слова или дистанции между словами в последовательностях.  \n",
    "<img src='https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NHCyyom7Qbu"
   },
   "source": [
    "### Декодер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Aobe3j65TKT"
   },
   "source": [
    "Выход последнего энкодера трансформируется в набор векторов внимания, они используются в декодере в слое внимания, который помогает декодеру фокусироваться на нужных местах во входных последовательностях.\n",
    "\n",
    "Такие шаги повторяются, пока не будет получен специальный символ, показывающий, что декодер закончил свой вывод. Выход каждого шага идет в следующий декодер и так же здесь, как и в части с энкодерами, добавляет позиционный векторы в начало декодирования для обозначения позиции каждого слова.\n",
    "\n",
    "<img src='https://jalammar.github.io/images/t/transformer_decoding_2.gif' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW09zJxHv92x"
   },
   "source": [
    "# BERT\n",
    "\n",
    "\n",
    "Идея в основе BERT лежит очень простая: давайте на вход нейросети будем подавать фразы, в которых 15% слов заменим на [MASK], и обучим нейронную сеть предсказывать эти закрытые маской слова.\n",
    "\n",
    "\n",
    "А для того, чтобы нейросеть научилась понимать соотношения между разными предложениями, дополнительно обучим ее предсказывать, является ли вторая фраза логичным продолжением первой. Или это какая-то случайная фраза, не имеющая никакого отношения к первой.\n",
    "\n",
    "\n",
    "Так, для двух предложений: \"Я пошел в магазин.\" и \"И купил там молоко.\", нейросеть должна ответить, что это логично. А если вторая фраза будет \"Карась небо Плутон\", то должна ответить, что это предложение никак не связано с первым.\n",
    "\n",
    "\n",
    "Обучив таким образом нейронную сеть на корпусе текстов из Wikipedia и сборнике книг BookCorpus в течении 4 дней на 16 TPU, получили BERT.\n",
    "\n",
    "\n",
    "\n",
    "Как это делается на практике:\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ENbcSICPN7YRmJKu3NvTBE1z9YLEtHMR\"/>\n",
    "\n",
    "\n",
    "Кодирует 15 процентов слов [MASK] и учим трансформеры + fully connected слои для классификации. Далее, формируем сет, где 50% предложений идут последовательно и 50% рандомно. Учим классификатор (заменяем первый и последний токены предложения, чтобы модель могла их отличить). Учится софтмакс\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1UUd8vjf7CbLoRhgALuUeE7jaCnLrnv5f\"/>\n",
    "\n",
    "\n",
    "Как проходит обучение:\n",
    "\n",
    "<img src='https://neerc.ifmo.ru/wiki/images/thumb/b/bc/BERT_pre-training.png/350px-BERT_pre-training.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT‑3 (Generative Pre‑trained Transformer 3) \n",
    "\n",
    "Обучена на 500 миллиардах слов и включает 175 миллиардов параметров. Модель способна качественно генерировать целые статьи и развёрнуто отвечать на вопросы, в том числе и достаточно узкоспециализированные, например из области генетики, а также анализировать строки, делая выводы из неструктурированных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1658255069266,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "8srnz4mgxrFt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4401,
     "status": "ok",
     "timestamp": 1658255075940,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "RUBVMndoxt9v",
    "outputId": "c3208993-af4e-47d4-e196-a704afe520cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mariya/PycharmProjects/base/venv/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12849,
     "status": "ok",
     "timestamp": 1658255135623,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "QKmlXQaP6-od",
    "outputId": "e1d5070f-7765-4b7c-f054-98407940ea5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5470118522644043,\n",
       "  'token': 5514,\n",
       "  'token_str': 'faster',\n",
       "  'sequence': 'pytorch is faster than tensorflow.'},\n",
       " {'score': 0.13801756501197815,\n",
       "  'token': 12430,\n",
       "  'token_str': 'slower',\n",
       "  'sequence': 'pytorch is slower than tensorflow.'},\n",
       " {'score': 0.06793206930160522,\n",
       "  'token': 16325,\n",
       "  'token_str': 'simpler',\n",
       "  'sequence': 'pytorch is simpler than tensorflow.'},\n",
       " {'score': 0.04572606459259987,\n",
       "  'token': 3760,\n",
       "  'token_str': 'smaller',\n",
       "  'sequence': 'pytorch is smaller than tensorflow.'},\n",
       " {'score': 0.04492785036563873,\n",
       "  'token': 3469,\n",
       "  'token_str': 'larger',\n",
       "  'sequence': 'pytorch is larger than tensorflow.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Pytorch is [MASK] than Tensorflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0lRd7XS-PGh"
   },
   "source": [
    "https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13962,
     "status": "ok",
     "timestamp": 1658255157065,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "NFRt4QVW75sw",
    "outputId": "f26c8d72-7e8f-4bae-cf83-8027628222d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9979189038276672}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = pipeline(\"text-classification\", model='Skoltech/russian-inappropriate-messages')\n",
    "sentiment(\"Этот ресторан очень плохой\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udoxIevPsWM3"
   },
   "source": [
    "Данные на google drive: https://drive.google.com/file/d/1Mev_EEput0LlBj8MDHIJkBtahlJ6J901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4334,
     "status": "ok",
     "timestamp": 1658255167727,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "PC4gkpggxnTJ",
    "outputId": "b82d014c-0026-4880-b71c-b32ccf2520b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-20 19:44:25--  https://drive.google.com/uc?export=download&id=1Mev_EEput0LlBj8MDHIJkBtahlJ6J901\n",
      "Распознаётся drive.google.com (drive.google.com)… 64.233.164.194, 2a00:1450:4010:c0a::c2\n",
      "Подключение к drive.google.com (drive.google.com)|64.233.164.194|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 See Other\n",
      "Адрес: https://doc-14-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jdsc3ife8be28gmlaed1i3idk09nc9qp/1700498625000/14904333240138417226/*/1Mev_EEput0LlBj8MDHIJkBtahlJ6J901?e=download&uuid=a89ed30e-5770-4152-9af3-38602d4e480b [переход]\n",
      "Предупреждение: в HTTP шаблоны не поддерживаются.\n",
      "--2023-11-20 19:44:30--  https://doc-14-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jdsc3ife8be28gmlaed1i3idk09nc9qp/1700498625000/14904333240138417226/*/1Mev_EEput0LlBj8MDHIJkBtahlJ6J901?e=download&uuid=a89ed30e-5770-4152-9af3-38602d4e480b\n",
      "Распознаётся doc-14-c0-docs.googleusercontent.com (doc-14-c0-docs.googleusercontent.com)… 64.233.161.132, 173.194.73.132, 2a00:1450:4010:c05::84\n",
      "Подключение к doc-14-c0-docs.googleusercontent.com (doc-14-c0-docs.googleusercontent.com)|64.233.161.132|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 10733963 (10M) [application/zip]\n",
      "Сохранение в: «data.zip»\n",
      "\n",
      "data.zip            100%[===================>]  10,24M  9,15MB/s    за 1,1s    \n",
      "\n",
      "2023-11-20 19:44:31 (9,15 MB/s) - «data.zip» сохранён [10733963/10733963]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=1Mev_EEput0LlBj8MDHIJkBtahlJ6J901' -O data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47604,
     "status": "ok",
     "timestamp": 1658255219401,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "-VLEaj4_xpGs",
    "outputId": "506b2b6c-3c19-4f72-e905-e0d31e617d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\r\n",
      "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1658255220041,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "bzK7ACWwv922",
    "outputId": "c2aa6d8a-1481-42b9-868b-1f9951401acc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((181467, 3), (22683, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_val = pd.read_csv(\"val.csv\")\n",
    "\n",
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1658255223720,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "tz1KrjOfsyoW",
    "outputId": "6551f84b-0ee2-4fea-9cab-f0e058206cf5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  class\n",
       "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1658255227963,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "W8SGuVDssuo0",
    "outputId": "e1ed6a96-8c16-4f4d-dfdd-4f1d1aa2aefc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    92063\n",
       "0    89404\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1658255230021,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "tP97qklX9E_N",
    "outputId": "6f30ab88-b2ac-488e-c957-0c49cb094719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мартовские путёвки дорожают на глазах. только пару дней назад были за 66, уже 86 о_О\n",
      "label is 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9420949816703796}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 11\n",
    "print(df_train.iloc[idx]['text'])\n",
    "print('label is', df_train.iloc[idx]['class'])\n",
    "sentiment(df_train.iloc[idx]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1658255240197,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "rMi0B4-qzjS3"
   },
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(lambda x: x.lower())\n",
    "df_val['text'] = df_val['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg7QTCpE83H6"
   },
   "source": [
    "- input_ids - закодированные айди токенов\n",
    "- attention_mask нужна для того, чтобы PAD символы не проходили через трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4620,
     "status": "ok",
     "timestamp": 1658255246959,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "sjWMGt2cv927",
    "outputId": "aa7e2cfa-1b65-4ee2-983c-7609b3bbda53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 14337, 33930, 77572, 10520, 84964, 15065, 33917,   102,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "example_text = 'Пример текста для токенизации'\n",
    "bert_input = tokenizer(example_text, padding='max_length', max_length=10, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "print(bert_input['input_ids'])\n",
    "print(bert_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1658254741948,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "lnkF_vjiWP4e",
    "outputId": "b7daadc7-e4cb-4708-beef-28196f381d76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('При', 'текста', '##мер', 'При')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_tokens[14337], tokenizer.ids_to_tokens[77572], tokenizer.ids_to_tokens[33930], tokenizer.ids_to_tokens[14337]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B2zNergX6Rn9"
   },
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1658255252881,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "oUhvnwe_v93B",
    "outputId": "09df13e1-d878-4f24-fdb3-ca6ce60268b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Пример текста для токенизации [SEP] [PAD]\n"
     ]
    }
   ],
   "source": [
    "example_text = tokenizer.decode(bert_input.input_ids[0])\n",
    "\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPu9-hjw6kC1"
   },
   "source": [
    "Препроцессинг для берта можно не делать, т.к. он самостоятельно умеет работать со знаками препинания, с разными регистрами и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1658255287508,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "OSNmvR6pv93E"
   },
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, txts, labels):\n",
    "        self._labels = labels\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self._txts = [self.tokenizer(text, padding='max_length', max_length=10,\n",
    "                                     truncation=True, return_tensors=\"pt\")\n",
    "                      for text in txts]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._txts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self._txts[index], self._labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 116256,
     "status": "ok",
     "timestamp": 1658255437346,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "jfslhXcwv93I"
   },
   "outputs": [],
   "source": [
    "y_train = df_train['class'].values\n",
    "y_val = df_val['class'].values\n",
    "\n",
    "train_dataset = TwitterDataset(df_train['text'], y_train)\n",
    "valid_dataset = TwitterDataset(df_val['text'], y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=64,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=64,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1658255508758,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "bjvZSIwM7iOE",
    "outputId": "b674804b-9f02-40dd-b29a-13343f64c7b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "torch.Size([64, 1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for txt, lbl in train_loader:\n",
    "    print(txt.keys())\n",
    "    print(txt['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1658255437824,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "KwCOq9Jzv93N"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        _, pooled_output = self.bert(input_ids=x, attention_mask=mask, return_dict=False)\n",
    "        # _, pooled_output - набор эмбеддинигов слов, эмбеддинг предложения\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.sigm(linear_output)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1658255557096,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "NGIXCpx-7_0v",
    "outputId": "4130d72a-e089-47bc-ce21-37b981086f2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "f1491d59495b4fde921acc945fa82fcd",
      "8d2886dbc98f4cd294ebc914bbd9774f",
      "9239105567ee4a07a275b172a2df1110",
      "ccc5add211c24b3a9a552a7cae278f54",
      "f61d661596dc454db0ff9e5b3088581c",
      "bbb9e65a00654372b06f0ae600619ab4",
      "d3d8b767ec3945a3a9a71075e552d093",
      "bdb42edb9deb40649e5f3462212e2f99",
      "dc03347b38ac46d4a914fb98c7542611",
      "b592deca53424987bce7741b7e3212bc",
      "19e201c55fd44b5c83315902a3975bb5"
     ]
    },
    "executionInfo": {
     "elapsed": 40145,
     "status": "ok",
     "timestamp": 1658255600662,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "6IKpinqqyVF-",
    "outputId": "39b49a91-33fc-405b-e605-be020f48782a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)  # полное обучение\n",
    "optimizer = Adam(model.linear.parameters(), lr=0.001)  # неполное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1658255642884,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "XJsGT0FX8XHl",
    "outputId": "ad19926f-f75f-4dde-866f-b079bd8bde8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (sigm): Sigmoid()\n",
      ")\n",
      "Parameters full train: 177854978\n",
      "Parameters transfer learning: 1538\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(\"Parameters full train:\", sum([param.nelement() for param in model.parameters()]))\n",
    "print(\"Parameters transfer learning:\", sum([param.nelement() for param in model.linear.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687793,
     "status": "ok",
     "timestamp": 1658256341437,
     "user": {
      "displayName": "Mariia Korliakova",
      "userId": "14165537686129864864"
     },
     "user_tz": -180
    },
    "id": "fsgnJX56v93Q",
    "outputId": "958bc64b-d6aa-4447-87b2-70c080892f0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2836 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████| 2836/2836 [02:24<00:00, 19.62it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.011         | Train Accuracy:  0.567         | Val Loss:  0.011         | Val Accuracy:  0.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2836 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████| 2836/2836 [02:23<00:00, 19.81it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.011         | Train Accuracy:  0.574         | Val Loss:  0.011         | Val Accuracy:  0.583\n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(2):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    model.train()\n",
    "    for train_input, train_label in tqdm(train_loader):\n",
    "        mask = train_input['attention_mask'].to(device)\n",
    "        input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "        train_label = train_label.to(device)\n",
    "\n",
    "        output = model(input_id, mask)\n",
    "                \n",
    "        batch_loss = criterion(output, train_label)\n",
    "        total_loss_train += batch_loss.item()\n",
    "                \n",
    "        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "        total_acc_train += acc\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    model.eval()\n",
    "    total_loss_val, total_acc_val = 0.0, 0.0\n",
    "    for val_input, val_label in valid_loader:\n",
    "        val_label = val_label.to(device)\n",
    "        mask = val_input['attention_mask'].to(device)\n",
    "        input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        output = model(input_id, mask)\n",
    "\n",
    "        batch_loss = criterion(output, val_label)\n",
    "        total_loss_val += batch_loss.item()\n",
    "                    \n",
    "        acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "        total_acc_val += acc\n",
    "            \n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataset): .3f} \\\n",
    "        | Train Accuracy: {total_acc_train / len(train_dataset): .3f} \\\n",
    "        | Val Loss: {total_loss_val / len(valid_dataset): .3f} \\\n",
    "        | Val Accuracy: {total_acc_val / len(valid_dataset): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5QxjZh1v93V"
   },
   "source": [
    "## Домашнее задание\n",
    "\n",
    "1. Возьмите готовую модель из https://huggingface.co/models для классификации сентимента текста.\n",
    "2. Сделайте предсказания на всем df_val. Посчитайте метрику качества.\n",
    "3. Дообучите эту модель на df_train. Посчитайте метрику качества на df_val.\n",
    "\n",
    "Данные на google drive: https://drive.google.com/file/d/1Mev_EEput0LlBj8MDHIJkBtahlJ6J901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные материалы:\n",
    " Фреймворк для упрощения обучения моделей (аналог keras): https://github.com/Lightning-AI/lightning\n",
    "\n",
    "Аугментации : https://albumentations.ai/docs/api_reference/pytorch/transforms/\n",
    "\n",
    "Метрики : https://github.com/Lightning-AI/torchmetrics\n",
    "\n",
    "ИНСТРУКЦИИ ПО ПОТЕРЯМ : https://neptune.ai/blog/pytorch-loss-functions\n",
    "\n",
    "Подбор гиперпараметров Optuna (Pytorch) : https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py\n",
    "\n",
    "Optuna общий гайд : https://optuna.readthedocs.io/en/stable/tutorial/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "web_9.ipynb",
   "provenance": [
    {
     "file_id": "1bA9okW91EkDoXWBObk4IIY40h8sbRbic",
     "timestamp": 1658231187246
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19e201c55fd44b5c83315902a3975bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d2886dbc98f4cd294ebc914bbd9774f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbb9e65a00654372b06f0ae600619ab4",
      "placeholder": "​",
      "style": "IPY_MODEL_d3d8b767ec3945a3a9a71075e552d093",
      "value": "Downloading: 100%"
     }
    },
    "9239105567ee4a07a275b172a2df1110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdb42edb9deb40649e5f3462212e2f99",
      "max": 714314041,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc03347b38ac46d4a914fb98c7542611",
      "value": 714314041
     }
    },
    "b592deca53424987bce7741b7e3212bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbb9e65a00654372b06f0ae600619ab4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdb42edb9deb40649e5f3462212e2f99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccc5add211c24b3a9a552a7cae278f54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b592deca53424987bce7741b7e3212bc",
      "placeholder": "​",
      "style": "IPY_MODEL_19e201c55fd44b5c83315902a3975bb5",
      "value": " 681M/681M [00:35&lt;00:00, 28.9MB/s]"
     }
    },
    "d3d8b767ec3945a3a9a71075e552d093": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc03347b38ac46d4a914fb98c7542611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1491d59495b4fde921acc945fa82fcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d2886dbc98f4cd294ebc914bbd9774f",
       "IPY_MODEL_9239105567ee4a07a275b172a2df1110",
       "IPY_MODEL_ccc5add211c24b3a9a552a7cae278f54"
      ],
      "layout": "IPY_MODEL_f61d661596dc454db0ff9e5b3088581c"
     }
    },
    "f61d661596dc454db0ff9e5b3088581c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
