{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb70532-9d41-4716-828a-8a0b4c6d8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#pip install -U sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9386ad-8745-47db-ad35-cd98589bc684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/home/alex/dev/AiLearning/DataSetStore/twitter_msg_sentiment_anal/train.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a05b1b1-ff1c-42c5-ba29-113404ce8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140, 10507,  9468,  9468,\n",
      "           102]]), 'token_type_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "FunnelBaseModel(\n",
      "  (embeddings): FunnelEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): FunnelEncoder(\n",
      "    (attention_structure): FunnelAttentionStructure(\n",
      "      (sin_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (cos_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-2): 3 x ModuleList(\n",
      "        (0-3): 4 x FunnelLayer(\n",
      "          (attention): FunnelRelMultiheadAttention(\n",
      "            (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_head): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (k_head): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_head): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (post_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
      "          )\n",
      "          (ffn): FunnelPositionwiseFFN(\n",
      "            (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (activation_function): NewGELUActivation()\n",
      "            (activation_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/funnel\n",
    "\n",
    "from transformers import AutoTokenizer, FunnelBaseModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"funnel-transformer/small-base\")\n",
    "model = FunnelBaseModel.from_pretrained(\"funnel-transformer/small-base\")\n",
    "inputs = tokenizer(\"Hello, my dog is cute cccccc\", return_tensors=\"pt\")\n",
    "#outputs = model(**inputs)\n",
    "\n",
    "#last_hidden_states = outputs.logits\n",
    "print (inputs)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f319106-6ec2-49e9-a17d-74365ce855d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, FSMTModel\n",
    "\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt19-ru-en\")\n",
    "\n",
    "# model = FSMTModel.from_pretrained(\"facebook/wmt19-ru-en\")\n",
    "# inputs = tokenizer(\"Hello, \", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "# print (last_hidden_states.shape)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f12ac3f-a583-4e9f-ad26-6ecdd4e5b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"funnel-transformer/small-base\")\n",
    "model = FunnelBaseModel.from_pretrained(\"funnel-transformer/small-base\")\n",
    "\n",
    "def funnel_encoder (txt: str, verbose=True, new_size=None):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(txt, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        result = outputs.last_hidden_state.numpy().copy()\n",
    "        result = result.reshape(result.shape[0]*result.shape[1]*result.shape[2])\n",
    "        zero = result\n",
    "        if new_size is not None:\n",
    "            zero = np.zeros(new_size)\n",
    "            for i in range(min(result.shape[0],new_size)):\n",
    "                zero[i]=zero[i]+result[i]\n",
    "            zero = zero.astype('float32')\n",
    "    \n",
    "    if verbose:\n",
    "        print (\"Predicting: {} {}\".format(txt,zero))\n",
    "    return zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b84052b9-5a5d-48f0-bf1d-0907c77b8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: Hellow world  ccc [-0.0286085  -0.03808539  0.00674396 ...  0.          0.\n",
      "  0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7680"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE = 7680\n",
    "funnel_encoder(txt=\"Hellow world  ccc\",new_size=INPUT_SIZE).shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ea2c644-cd1a-47fa-8538-77f646be6d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31962it [16:59, 31.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 7680) (31962,)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "for (twt,lab) in tqdm(zip(df['tweet'],df['label'])):\n",
    "    x = funnel_encoder(txt=twt,new_size=INPUT_SIZE,verbose=False)\n",
    "    if x.shape[0] <= INPUT_SIZE:\n",
    "        X.append(x)\n",
    "        Y.append(lab)\n",
    "    else:\n",
    "        print (\"warn: \",x.shape)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print (X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "57542d6f-21b6-4655-870b-4385875ee763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 7680) (31962,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X).astype('float32')\n",
    "Y = np.array(Y).astype('float32')\n",
    "\n",
    "print (X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a4d7441-209d-4bb8-ae45-c35973eeb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, text_list, label_list):\n",
    "        self.text_list = text_list\n",
    "        self.label_list = label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text_list[index],self.label_list[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "606f94c6-ecb5-4a40-9344-5b7f4a235034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Let's work with cuda\n",
      "  --> Let's work with cuda\n",
      "  --> Let's work with cuda\n",
      "  --> Let's work with cuda\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([1000, 7680])\n",
      "  --> Copied data:  torch.Size([1000])\n",
      "  --> Copied data:  torch.Size([962, 7680])\n",
      "  --> Copied data:  torch.Size([962])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def execute (runnable):\n",
    "    if torch.cuda.device_count()>0:\n",
    "        print (\"  --> Let's work with cuda\")\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if runnable is not None:\n",
    "            with torch.cuda.device(0):\n",
    "                return runnable(device)\n",
    "        return device\n",
    "    else:\n",
    "        print (\"  --> Let's work with CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        if runnable is not None:\n",
    "            return runnable(device)\n",
    "        return device\n",
    "\n",
    "for x in range (3):\n",
    "    execute(None)\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    print (\"  --> Copied data: \",data.shape)\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "def load_to_device(loader,device):\n",
    "    load_list=[]\n",
    "    for i, data in enumerate(loader):\n",
    "        try:\n",
    "            load_list.append((to_device(data[0],device), to_device(data[1],device)))\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "    return load_list\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "train_dataset = MyDataset(text_list=X,label_list=Y)\n",
    "\n",
    "# for (i,(x,y)) in enumerate(train_dataset):\n",
    "#     if i%100==0:\n",
    "#         print (x,y)\n",
    "\n",
    "train_set_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "train_list = load_to_device(train_set_loader,execute(None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7157742c-afc2-4dda-9506-7c8f3e996778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, hidden_layer_quantity, batch_norm_enabled=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        \n",
    "        # Making the basic perceptron:---------------------------------\n",
    "        dropout_base = 0.05\n",
    "        self.layers.append(nn.Linear(input_size,hidden_size))\n",
    "        self.layers.append(nn.Dropout( dropout_base ))\n",
    "        \n",
    "        for cnt in range(0,hidden_layer_quantity):\n",
    "            self.layers.append(nn.Linear(hidden_size,hidden_size))\n",
    "            if batch_norm_enabled == True:\n",
    "                self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            drop_coef = (1.0/hidden_layer_quantity) * cnt + 2*dropout_base\n",
    "            drop_coef = 0.80 if drop_coef>0.80 else drop_coef\n",
    "            self.layers.append(nn.Dropout( drop_coef ))\n",
    "        self.layers.append(nn.Linear(hidden_size,output_size))\n",
    "\n",
    "        # Registering modules for torch:-------------------------------\n",
    "        for cnt,layer in enumerate(self.layers):\n",
    "            key = \"layer{}\".format(cnt)\n",
    "            self.add_module(key,layer)\n",
    "\n",
    "    def forward(self, xtr):\n",
    "        for cnt,layer in enumerate(self.layers):\n",
    "            if cnt<len(self.layers)-1:\n",
    "                xtr = F.leaky_relu(layer(xtr))\n",
    "            else :\n",
    "                xtr = F.sigmoid(layer(xtr))\n",
    "        return xtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef8d1c76-b50b-44c2-a15f-ebfd59a3205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Let's work with cuda\n",
      "0 -> Perceptron(\n",
      "  (layer0): Linear(in_features=7680, out_features=300, bias=True)\n",
      "  (layer1): Dropout(p=0.05, inplace=False)\n",
      "  (layer2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (layer3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer4): Dropout(p=0.1, inplace=False)\n",
      "  (layer5): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (layer6): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer7): Dropout(p=0.43333333333333335, inplace=False)\n",
      "  (layer8): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (layer9): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer10): Dropout(p=0.7666666666666666, inplace=False)\n",
      "  (layer11): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "1 -> Linear(in_features=7680, out_features=300, bias=True)\n",
      "2 -> Dropout(p=0.05, inplace=False)\n",
      "3 -> Linear(in_features=300, out_features=300, bias=True)\n",
      "4 -> BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5 -> Dropout(p=0.1, inplace=False)\n",
      "6 -> Linear(in_features=300, out_features=300, bias=True)\n",
      "7 -> BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "8 -> Dropout(p=0.43333333333333335, inplace=False)\n",
      "9 -> Linear(in_features=300, out_features=300, bias=True)\n",
      "10 -> BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "11 -> Dropout(p=0.7666666666666666, inplace=False)\n",
      "12 -> Linear(in_features=300, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "perceptron = Perceptron(INPUT_SIZE,300,1,3).to(execute(None))\n",
    "\n",
    "for idx, module in enumerate(perceptron.modules()):\n",
    "     print(idx, '->', module)\n",
    "\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = optim.SGD(perceptron.parameters(), lr=0.005, momentum=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96a0f32b-3a74-4de6-9de4-af0d9a77bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> Let's work with cuda\n",
      "Epoch [0/100000] loss: 7.724609360098839\n",
      "Epoch [1000/100000] loss: 2.160012736916542\n",
      "Epoch [2000/100000] loss: 2.140989728271961\n",
      "Epoch [3000/100000] loss: 2.1292636804282665\n",
      "Epoch [4000/100000] loss: 2.1215590313076973\n",
      "Epoch [5000/100000] loss: 2.1142961494624615\n",
      "Epoch [6000/100000] loss: 2.108599830418825\n",
      "Epoch [7000/100000] loss: 2.1046392619609833\n",
      "Epoch [8000/100000] loss: 2.1010694429278374\n",
      "Epoch [9000/100000] loss: 2.0984586887061596\n",
      "Epoch [10000/100000] loss: 2.0962159894406796\n",
      "Epoch [11000/100000] loss: 2.0944766625761986\n",
      "Epoch [12000/100000] loss: 2.0931642167270184\n",
      "Epoch [13000/100000] loss: 2.0919413790106773\n",
      "Epoch [14000/100000] loss: 2.0911302231252193\n",
      "Epoch [15000/100000] loss: 2.090341307222843\n",
      "Epoch [16000/100000] loss: 2.0898041538894176\n",
      "Epoch [17000/100000] loss: 2.0893528535962105\n",
      "Epoch [18000/100000] loss: 2.088976301252842\n",
      "Epoch [19000/100000] loss: 2.088675446808338\n",
      "Epoch [20000/100000] loss: 2.088412892073393\n",
      "Epoch [21000/100000] loss: 2.088212836533785\n",
      "Epoch [22000/100000] loss: 2.0880558118224144\n",
      "Epoch [23000/100000] loss: 2.0879290141165257\n",
      "Epoch [24000/100000] loss: 2.0878090746700764\n",
      "Epoch [25000/100000] loss: 2.087717168033123\n",
      "Epoch [26000/100000] loss: 2.087648782879114\n",
      "Epoch [27000/100000] loss: 2.0875867903232574\n",
      "Epoch [28000/100000] loss: 2.0875541865825653\n",
      "Epoch [29000/100000] loss: 2.087512604892254\n",
      "Epoch [30000/100000] loss: 2.087472766637802\n",
      "Epoch [31000/100000] loss: 2.0874539129436016\n",
      "Epoch [32000/100000] loss: 2.087437216192484\n",
      "Epoch [33000/100000] loss: 2.087410755455494\n",
      "Epoch [34000/100000] loss: 2.087399709969759\n",
      "Epoch [35000/100000] loss: 2.0873820893466473\n",
      "Epoch [36000/100000] loss: 2.087379325181246\n",
      "Epoch [37000/100000] loss: 2.0873704999685287\n",
      "Epoch [38000/100000] loss: 2.0873622223734856\n",
      "Epoch [39000/100000] loss: 2.087357744574547\n",
      "Epoch [40000/100000] loss: 2.0873542614281178\n",
      "Epoch [41000/100000] loss: 2.087348710745573\n",
      "Epoch [42000/100000] loss: 2.087346166372299\n",
      "Epoch [43000/100000] loss: 2.0873468667268753\n",
      "Epoch [44000/100000] loss: 2.087344117462635\n",
      "Epoch [45000/100000] loss: 2.0873417742550373\n",
      "Epoch [46000/100000] loss: 2.0873414501547813\n",
      "Epoch [47000/100000] loss: 2.0873395316302776\n",
      "Epoch [48000/100000] loss: 2.087340246886015\n",
      "Epoch [49000/100000] loss: 2.0873371101915836\n",
      "Epoch [50000/100000] loss: 2.087336555123329\n",
      "Epoch [51000/100000] loss: 2.0873364955186844\n",
      "Epoch [52000/100000] loss: 2.0873376317322254\n",
      "Epoch [53000/100000] loss: 2.0873370468616486\n",
      "Epoch [54000/100000] loss: 2.087335232645273\n",
      "Epoch [55000/100000] loss: 2.087335929274559\n",
      "Epoch [56000/100000] loss: 2.087335739284754\n",
      "Epoch [57000/100000] loss: 2.0873348899185658\n",
      "Epoch [58000/100000] loss: 2.087335053831339\n",
      "Epoch [59000/100000] loss: 2.087335206568241\n",
      "Epoch [60000/100000] loss: 2.0873350873589516\n",
      "Epoch [61000/100000] loss: 2.087334770709276\n",
      "Epoch [62000/100000] loss: 2.087334703654051\n",
      "Epoch [63000/100000] loss: 2.0873347483575344\n",
      "Epoch [64000/100000] loss: 2.0873348973691463\n",
      "Epoch [65000/100000] loss: 2.0873347222805023\n",
      "Epoch [66000/100000] loss: 2.087334431707859\n",
      "Epoch [67000/100000] loss: 2.0873345099389553\n",
      "Epoch [68000/100000] loss: 2.087334554642439\n",
      "Epoch [69000/100000] loss: 2.0873347371816635\n",
      "Epoch [70000/100000] loss: 2.0873344615101814\n",
      "Epoch [71000/100000] loss: 2.087334595620632\n",
      "Epoch [72000/100000] loss: 2.087334543466568\n",
      "Epoch [73000/100000] loss: 2.087334632873535\n",
      "Epoch [74000/100000] loss: 2.087334606796503\n",
      "Epoch [75000/100000] loss: 2.087334655225277\n",
      "Epoch [76000/100000] loss: 2.087334644049406\n",
      "Epoch [77000/100000] loss: 2.0873345471918583\n",
      "Epoch [78000/100000] loss: 2.087334591895342\n",
      "Epoch [79000/100000] loss: 2.0873346142470837\n",
      "Epoch [80000/100000] loss: 2.087334506213665\n",
      "Epoch [81000/100000] loss: 2.08733469247818\n",
      "Epoch [82000/100000] loss: 2.087334629148245\n",
      "Epoch [83000/100000] loss: 2.087334644049406\n",
      "Epoch [84000/100000] loss: 2.0873346738517284\n",
      "Epoch [85000/100000] loss: 2.0873346105217934\n",
      "Epoch [86000/100000] loss: 2.0873345360159874\n",
      "Epoch [87000/100000] loss: 2.087334495037794\n",
      "Epoch [88000/100000] loss: 2.0873346589505672\n",
      "Epoch [89000/100000] loss: 2.0873346962034702\n",
      "Epoch [90000/100000] loss: 2.0873346999287605\n",
      "Epoch [91000/100000] loss: 2.0873345732688904\n",
      "Epoch [92000/100000] loss: 2.0873345509171486\n",
      "Epoch [93000/100000] loss: 2.0873346962034702\n",
      "Epoch [94000/100000] loss: 2.087334703654051\n",
      "Epoch [95000/100000] loss: 2.087334804236889\n",
      "Epoch [96000/100000] loss: 2.087334807962179\n",
      "Epoch [97000/100000] loss: 2.087334495037794\n",
      "Epoch [98000/100000] loss: 2.0873346365988255\n",
      "Epoch [99000/100000] loss: 2.087334632873535\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def adapt_perceptron (device):\n",
    "    epoch_max_num = 100000\n",
    "    history=[]\n",
    "    for epoch_num in range(epoch_max_num):\n",
    "        running_loss = 0.0\n",
    "        for train_batch in train_list:\n",
    "            x_train, y_train = train_batch[0], train_batch[1]\n",
    "            #y_train[y_train==1.0]=0.75\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = perceptron(x_train)\n",
    "            loss = criteria(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        history.append((epoch_num,running_loss))\n",
    "        if epoch_num % 1000==0 :\n",
    "            print(\"Epoch [{}/{}] loss: {}\".format(epoch_num,epoch_max_num,running_loss))\n",
    "\n",
    "    return history\n",
    "\n",
    "train_history = execute(runnable=adapt_perceptron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e0236-d733-4cfa-afa8-f02bc3299163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
